{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anirrudh Ramesh Nov 20 2020\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from tensorflow . python . training . moving_averages \\\n",
    "import assign_moving_average\n",
    "from scipy . stats import multivariate_normal as normal\n",
    "\n",
    "from tensorflow . python . ops import control_flow_ops\n",
    "from tensorflow import random_normal_initializer as norm_init\n",
    "from tensorflow import random_uniform_initializer as unif_init\n",
    "from tensorflow import constant_initializer as const_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to solve approximation\n",
      "end of build\n",
      "before initialized\n",
      "step:     0, loss: 4.6399e+02,\n",
      "step:   100, loss: 6.1844e+00, time:     1\n",
      "step:   200, loss: 1.8112e-01, time:     2\n",
      "step:   300, loss: 3.3167e-01, time:     2\n",
      "step:   400, loss: 3.1772e-02, time:     3\n",
      "step:   500, loss: 6.1607e-02, time:     4\n",
      "step:   600, loss: 2.0642e-02, time:     5\n",
      "step:   700, loss: 1.6130e-02, time:     6\n",
      "step:   800, loss: 5.8673e-02, time:     6\n",
      "step:   900, loss: 1.2456e-01, time:     7\n",
      "step:  1000, loss: 1.7092e-02, time:     8\n",
      "step:  1100, loss: 2.2557e-01, time:     9\n",
      "step:  1200, loss: 6.6608e-01, time:    10\n",
      "step:  1300, loss: 1.4885e-02, time:    10\n",
      "step:  1400, loss: 5.1758e-02, time:    11\n",
      "step:  1500, loss: 1.1212e-02, time:    12\n",
      "step:  1600, loss: 1.3982e-01, time:    13\n",
      "step:  1700, loss: 2.4900e-02, time:    14\n",
      "step:  1800, loss: 1.0663e-02, time:    15\n",
      "step:  1900, loss: 4.4203e-02, time:    16\n",
      "step:  2000, loss: 1.2866e-02, time:    16\n",
      "step:  2100, loss: 1.1972e-02, time:    17\n",
      "step:  2200, loss: 4.1221e-02, time:    18\n",
      "step:  2300, loss: 4.8041e+00, time:    19\n",
      "step:  2400, loss: 1.8205e-02, time:    20\n",
      "step:  2500, loss: 1.0246e-02, time:    21\n",
      "step:  2600, loss: 8.2721e-03, time:    21\n",
      "step:  2700, loss: 9.4288e-02, time:    22\n",
      "step:  2800, loss: 4.4038e-01, time:    23\n",
      "step:  2900, loss: 1.0299e+00, time:    24\n",
      "step:  3000, loss: 8.6046e-03, time:    25\n",
      "step:  3100, loss: 1.2994e-01, time:    25\n",
      "step:  3200, loss: 1.8591e-01, time:    26\n",
      "step:  3300, loss: 8.1720e-03, time:    27\n",
      "step:  3400, loss: 3.5290e-02, time:    28\n",
      "step:  3500, loss: 6.7690e-01, time:    29\n",
      "step:  3600, loss: 8.3374e-03, time:    30\n",
      "step:  3700, loss: 5.3465e-03, time:    30\n",
      "step:  3800, loss: 8.8216e-03, time:    31\n",
      "step:  3900, loss: 7.3753e-02, time:    32\n",
      "step:  4000, loss: 1.2179e-02, time:    33\n",
      "step:  4100, loss: 8.1098e-03, time:    34\n",
      "step:  4200, loss: 6.0627e-03, time:    34\n",
      "step:  4300, loss: 1.0637e-02, time:    35\n",
      "step:  4400, loss: 1.6855e-02, time:    36\n",
      "step:  4500, loss: 4.3169e-03, time:    37\n",
      "step:  4600, loss: 1.2938e-02, time:    38\n",
      "step:  4700, loss: 3.1518e-02, time:    39\n",
      "step:  4800, loss: 3.6293e-01, time:    39\n",
      "step:  4900, loss: 1.8401e-01, time:    40\n",
      "step:  5000, loss: 1.1503e-01, time:    41\n",
      "step:  5100, loss: 7.4739e-03, time:    42\n",
      "step:  5200, loss: 3.3718e-01, time:    43\n",
      "step:  5300, loss: 3.3861e-01, time:    44\n",
      "step:  5400, loss: 6.4422e-01, time:    45\n",
      "step:  5500, loss: 3.9642e-03, time:    45\n",
      "step:  5600, loss: 3.1814e-03, time:    46\n",
      "step:  5700, loss: 8.3403e-02, time:    47\n",
      "step:  5800, loss: 3.6948e-03, time:    48\n",
      "step:  5900, loss: 5.5208e-03, time:    49\n",
      "step:  6000, loss: 1.2974e-02, time:    50\n",
      "step:  6100, loss: 1.2064e-01, time:    50\n",
      "step:  6200, loss: 3.1622e-02, time:    51\n",
      "step:  6300, loss: 3.3648e-03, time:    52\n",
      "step:  6400, loss: 2.7986e-03, time:    53\n",
      "step:  6500, loss: 2.9470e-03, time:    54\n",
      "step:  6600, loss: 3.5681e-03, time:    54\n",
      "step:  6700, loss: 5.2310e-03, time:    55\n",
      "step:  6800, loss: 5.5451e-03, time:    56\n",
      "step:  6900, loss: 8.9347e-03, time:    57\n",
      "step:  7000, loss: 6.7693e-03, time:    58\n",
      "step:  7100, loss: 4.3444e-03, time:    59\n",
      "step:  7200, loss: 1.0983e-01, time:    59\n",
      "step:  7300, loss: 4.7531e-03, time:    60\n",
      "step:  7400, loss: 7.0400e-03, time:    61\n",
      "step:  7500, loss: 7.0176e-03, time:    62\n",
      "step:  7600, loss: 1.8452e-02, time:    63\n",
      "step:  7700, loss: 5.2635e-03, time:    63\n",
      "step:  7800, loss: 1.6869e-03, time:    64\n",
      "step:  7900, loss: 5.1989e-03, time:    65\n",
      "step:  8000, loss: 5.6076e-02, time:    66\n",
      "step:  8100, loss: 2.7644e-03, time:    67\n",
      "step:  8200, loss: 5.5950e-02, time:    67\n",
      "step:  8300, loss: 4.3508e-02, time:    68\n",
      "step:  8400, loss: 2.9484e-03, time:    69\n",
      "step:  8500, loss: 7.2927e-02, time:    70\n",
      "step:  8600, loss: 7.8058e-03, time:    71\n",
      "step:  8700, loss: 9.8814e-02, time:    71\n",
      "step:  8800, loss: 1.2185e-01, time:    72\n",
      "step:  8900, loss: 2.4497e-02, time:    73\n",
      "step:  9000, loss: 2.5899e-03, time:    74\n",
      "step:  9100, loss: 1.3298e-01, time:    75\n",
      "step:  9200, loss: 4.3565e-02, time:    76\n",
      "step:  9300, loss: 1.7907e-03, time:    76\n",
      "step:  9400, loss: 7.1759e-02, time:    77\n",
      "step:  9500, loss: 8.1008e-03, time:    78\n",
      "step:  9600, loss: 5.5370e-03, time:    79\n",
      "step:  9700, loss: 1.0055e-02, time:    80\n",
      "step:  9800, loss: 7.2230e-03, time:    81\n",
      "step:  9900, loss: 2.1316e-01, time:    81\n",
      "step: 10000, loss: 2.6839e-02, time:    82\n",
      "step: 10100, loss: 5.8887e-03, time:    83\n",
      "step: 10200, loss: 3.3852e-02, time:    84\n",
      "step: 10300, loss: 5.4404e-03, time:    85\n",
      "step: 10400, loss: 1.4251e-02, time:    85\n",
      "step: 10500, loss: 2.5031e-02, time:    86\n",
      "step: 10600, loss: 4.7302e-03, time:    87\n",
      "step: 10700, loss: 2.9277e-02, time:    88\n",
      "step: 10800, loss: 1.8650e-02, time:    89\n",
      "step: 10900, loss: 8.5524e-02, time:    90\n",
      "step: 11000, loss: 1.3532e-01, time:    90\n",
      "step: 11100, loss: 2.9074e-03, time:    91\n",
      "step: 11200, loss: 1.3990e-02, time:    92\n",
      "step: 11300, loss: 3.8152e-03, time:    93\n",
      "step: 11400, loss: 2.1899e-03, time:    94\n",
      "step: 11500, loss: 1.9074e-03, time:    94\n",
      "step: 11600, loss: 2.7337e-03, time:    95\n",
      "step: 11700, loss: 1.8828e-01, time:    96\n",
      "step: 11800, loss: 2.8978e-02, time:    97\n",
      "step: 11900, loss: 1.4283e+00, time:    98\n",
      "step: 12000, loss: 1.1317e-02, time:    98\n",
      "step: 12100, loss: 8.4805e-03, time:    99\n",
      "step: 12200, loss: 6.5555e-03, time:   100\n",
      "step: 12300, loss: 2.5806e-03, time:   101\n",
      "step: 12400, loss: 6.9960e-01, time:   102\n",
      "step: 12500, loss: 2.7933e-02, time:   102\n",
      "step: 12600, loss: 4.4156e-03, time:   103\n",
      "step: 12700, loss: 7.3147e-03, time:   104\n",
      "step: 12800, loss: 2.8796e-03, time:   105\n",
      "step: 12900, loss: 1.4598e-01, time:   106\n",
      "step: 13000, loss: 9.4978e-03, time:   106\n",
      "step: 13100, loss: 1.6240e-03, time:   107\n",
      "step: 13200, loss: 8.5418e-03, time:   108\n",
      "step: 13300, loss: 6.0941e-03, time:   109\n",
      "step: 13400, loss: 2.4388e-03, time:   110\n",
      "step: 13500, loss: 5.6905e-03, time:   111\n",
      "step: 13600, loss: 1.2110e-02, time:   111\n",
      "step: 13700, loss: 2.3027e+00, time:   112\n",
      "step: 13800, loss: 1.5219e-03, time:   113\n",
      "step: 13900, loss: 1.6270e-03, time:   114\n",
      "step: 14000, loss: 7.0330e-03, time:   115\n",
      "step: 14100, loss: 9.4541e-03, time:   115\n",
      "step: 14200, loss: 3.2187e-03, time:   116\n",
      "step: 14300, loss: 1.9974e-01, time:   117\n",
      "step: 14400, loss: 1.0736e-02, time:   118\n",
      "step: 14500, loss: 1.0698e-02, time:   118\n",
      "step: 14600, loss: 2.6753e-03, time:   119\n",
      "step: 14700, loss: 2.6216e-03, time:   120\n",
      "step: 14800, loss: 1.7696e-03, time:   121\n",
      "step: 14900, loss: 7.0153e-03, time:   122\n",
      "step: 15000, loss: 1.5685e-02, time:   122\n",
      "step: 15100, loss: 2.4076e-02, time:   123\n",
      "step: 15200, loss: 1.5028e-02, time:   124\n",
      "step: 15300, loss: 2.2927e-01, time:   125\n",
      "step: 15400, loss: 3.8204e-03, time:   126\n",
      "step: 15500, loss: 1.2232e-03, time:   126\n",
      "step: 15600, loss: 5.9715e-03, time:   127\n",
      "step: 15700, loss: 1.9049e-03, time:   128\n",
      "step: 15800, loss: 1.8875e-01, time:   129\n",
      "step: 15900, loss: 1.1046e-02, time:   130\n",
      "step: 16000, loss: 1.1474e-03, time:   130\n",
      "step: 16100, loss: 3.2924e-01, time:   131\n",
      "step: 16200, loss: 3.9498e-03, time:   132\n",
      "step: 16300, loss: 2.3393e-03, time:   133\n",
      "step: 16400, loss: 1.6356e-03, time:   134\n",
      "step: 16500, loss: 1.0302e-03, time:   135\n",
      "step: 16600, loss: 2.0392e-02, time:   136\n",
      "step: 16700, loss: 1.6729e-03, time:   136\n",
      "step: 16800, loss: 7.1153e-02, time:   137\n",
      "step: 16900, loss: 3.1603e-03, time:   138\n",
      "step: 17000, loss: 4.9960e-03, time:   139\n",
      "step: 17100, loss: 1.2350e-03, time:   140\n",
      "step: 17200, loss: 1.0953e-03, time:   141\n",
      "step: 17300, loss: 3.0286e-03, time:   141\n",
      "step: 17400, loss: 2.8963e-03, time:   142\n",
      "step: 17500, loss: 7.8629e-02, time:   143\n",
      "step: 17600, loss: 1.1614e-03, time:   144\n",
      "step: 17700, loss: 9.5358e-04, time:   145\n",
      "step: 17800, loss: 7.1290e-02, time:   146\n",
      "step: 17900, loss: 9.4327e-03, time:   147\n",
      "step: 18000, loss: 1.2464e-03, time:   148\n",
      "step: 18100, loss: 1.2425e-03, time:   148\n",
      "step: 18200, loss: 2.3240e-01, time:   149\n",
      "step: 18300, loss: 2.6651e-02, time:   150\n",
      "step: 18400, loss: 1.2325e-03, time:   151\n",
      "step: 18500, loss: 8.1313e-04, time:   152\n",
      "step: 18600, loss: 3.7435e-03, time:   152\n",
      "step: 18700, loss: 9.6646e-02, time:   153\n",
      "step: 18800, loss: 5.7636e-03, time:   154\n",
      "step: 18900, loss: 6.4048e-03, time:   155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19000, loss: 1.5671e-01, time:   156\n",
      "step: 19100, loss: 1.2135e-03, time:   156\n",
      "step: 19200, loss: 8.1219e-04, time:   157\n",
      "step: 19300, loss: 2.4079e-03, time:   158\n",
      "step: 19400, loss: 2.2792e-01, time:   159\n",
      "step: 19500, loss: 3.1843e-03, time:   160\n",
      "step: 19600, loss: 1.3759e-03, time:   160\n",
      "step: 19700, loss: 1.0041e-03, time:   161\n",
      "step: 19800, loss: 3.1487e-03, time:   162\n",
      "step: 19900, loss: 2.7604e-03, time:   163\n",
      "step: 20000, loss: 1.0327e-03, time:   164\n",
      "step: 20100, loss: 9.3020e-04, time:   164\n",
      "step: 20200, loss: 1.8160e-02, time:   165\n",
      "step: 20300, loss: 1.2342e-03, time:   166\n",
      "step: 20400, loss: 9.0783e-04, time:   167\n",
      "step: 20500, loss: 7.0117e-01, time:   168\n",
      "step: 20600, loss: 1.9771e-03, time:   168\n",
      "step: 20700, loss: 2.9667e-02, time:   169\n",
      "step: 20800, loss: 1.5020e-02, time:   170\n",
      "step: 20900, loss: 1.2289e-03, time:   171\n",
      "step: 21000, loss: 1.1515e-03, time:   172\n",
      "step: 21100, loss: 2.8686e-03, time:   172\n",
      "step: 21200, loss: 1.4515e-02, time:   173\n",
      "step: 21300, loss: 5.5907e-03, time:   174\n",
      "step: 21400, loss: 1.7726e-03, time:   175\n",
      "step: 21500, loss: 6.5923e-02, time:   175\n",
      "step: 21600, loss: 4.8810e-02, time:   176\n",
      "step: 21700, loss: 4.7504e-02, time:   177\n",
      "step: 21800, loss: 2.5250e-03, time:   178\n",
      "step: 21900, loss: 3.5348e-01, time:   178\n",
      "step: 22000, loss: 9.0097e-02, time:   179\n",
      "step: 22100, loss: 4.7202e-03, time:   180\n",
      "step: 22200, loss: 1.5859e-03, time:   181\n",
      "step: 22300, loss: 2.5059e-03, time:   181\n",
      "step: 22400, loss: 1.1594e-03, time:   182\n",
      "step: 22500, loss: 1.4425e-03, time:   183\n",
      "step: 22600, loss: 8.8534e-04, time:   184\n",
      "step: 22700, loss: 2.3514e-01, time:   185\n",
      "step: 22800, loss: 1.1706e-03, time:   185\n",
      "step: 22900, loss: 1.0196e-03, time:   186\n",
      "step: 23000, loss: 7.7904e-03, time:   187\n",
      "step: 23100, loss: 7.5722e-03, time:   188\n",
      "step: 23200, loss: 3.3152e-01, time:   189\n",
      "step: 23300, loss: 2.8077e-03, time:   189\n",
      "step: 23400, loss: 1.7544e-03, time:   190\n",
      "step: 23500, loss: 1.3027e-03, time:   191\n",
      "step: 23600, loss: 3.5688e-03, time:   192\n",
      "step: 23700, loss: 3.0942e-02, time:   193\n",
      "step: 23800, loss: 1.0746e-03, time:   194\n",
      "step: 23900, loss: 5.6554e-03, time:   194\n",
      "step: 24000, loss: 2.1511e-03, time:   195\n",
      "step: 24100, loss: 1.8634e-03, time:   196\n",
      "step: 24200, loss: 1.6130e-03, time:   197\n",
      "step: 24300, loss: 1.4414e-03, time:   197\n",
      "step: 24400, loss: 1.3010e-03, time:   198\n",
      "step: 24500, loss: 1.2400e-03, time:   199\n",
      "step: 24600, loss: 1.1697e-03, time:   200\n",
      "step: 24700, loss: 1.8284e-03, time:   200\n",
      "step: 24800, loss: 1.9855e-03, time:   201\n",
      "step: 24900, loss: 3.3971e-03, time:   202\n",
      "step: 25000, loss: 8.4672e-01, time:   203\n",
      "step: 25100, loss: 1.8384e-01, time:   204\n",
      "step: 25200, loss: 1.5367e-03, time:   204\n",
      "step: 25300, loss: 1.2140e-03, time:   205\n",
      "step: 25400, loss: 2.6223e-02, time:   206\n",
      "step: 25500, loss: 1.0019e-02, time:   207\n",
      "step: 25600, loss: 1.9473e-02, time:   207\n",
      "step: 25700, loss: 9.2134e-02, time:   208\n",
      "step: 25800, loss: 1.4710e-02, time:   209\n",
      "step: 25900, loss: 3.6647e-02, time:   210\n",
      "step: 26000, loss: 2.7925e-03, time:   210\n",
      "step: 26100, loss: 2.1717e-01, time:   211\n",
      "step: 26200, loss: 1.3641e-03, time:   212\n",
      "step: 26300, loss: 3.0582e-03, time:   213\n",
      "step: 26400, loss: 3.4468e-02, time:   213\n",
      "step: 26500, loss: 3.5761e-03, time:   214\n",
      "step: 26600, loss: 1.2113e-03, time:   215\n",
      "step: 26700, loss: 4.5145e-03, time:   216\n",
      "step: 26800, loss: 9.3078e-04, time:   217\n",
      "step: 26900, loss: 9.8598e-04, time:   217\n",
      "step: 27000, loss: 1.2936e-01, time:   218\n",
      "step: 27100, loss: 1.9296e-03, time:   219\n",
      "step: 27200, loss: 8.9175e-04, time:   220\n",
      "step: 27300, loss: 1.9606e-01, time:   221\n",
      "step: 27400, loss: 1.4472e-03, time:   222\n",
      "step: 27500, loss: 1.6140e-03, time:   223\n",
      "step: 27600, loss: 1.2053e-02, time:   223\n",
      "step: 27700, loss: 1.9549e-03, time:   224\n",
      "step: 27800, loss: 7.2027e-02, time:   225\n",
      "step: 27900, loss: 8.0558e-03, time:   226\n",
      "step: 28000, loss: 1.1111e-03, time:   227\n",
      "step: 28100, loss: 9.9374e-04, time:   228\n",
      "step: 28200, loss: 8.7392e-04, time:   228\n",
      "step: 28300, loss: 1.8344e-02, time:   229\n",
      "step: 28400, loss: 7.2920e-04, time:   230\n",
      "step: 28500, loss: 1.3052e-03, time:   231\n",
      "step: 28600, loss: 1.9707e-02, time:   232\n",
      "step: 28700, loss: 8.7633e-04, time:   232\n",
      "step: 28800, loss: 1.1357e-03, time:   233\n",
      "step: 28900, loss: 8.2471e-04, time:   234\n",
      "step: 29000, loss: 6.0807e-04, time:   235\n",
      "step: 29100, loss: 2.8100e-03, time:   236\n",
      "step: 29200, loss: 1.0385e-03, time:   236\n",
      "step: 29300, loss: 8.0588e-03, time:   237\n",
      "step: 29400, loss: 2.5579e-03, time:   238\n",
      "step: 29500, loss: 1.6307e-03, time:   239\n",
      "step: 29600, loss: 1.2922e-03, time:   240\n",
      "step: 29700, loss: 1.3830e-03, time:   240\n",
      "step: 29800, loss: 1.4547e-03, time:   241\n",
      "step: 29900, loss: 1.9820e-03, time:   242\n",
      "step: 30000, loss: 9.8013e-04, time:   243\n",
      "step: 30100, loss: 5.7945e-02, time:   243\n",
      "step: 30200, loss: 1.2542e-03, time:   244\n",
      "step: 30300, loss: 1.2268e-02, time:   245\n",
      "step: 30400, loss: 1.2138e-03, time:   246\n",
      "step: 30500, loss: 1.5459e-03, time:   247\n",
      "step: 30600, loss: 4.5268e-03, time:   248\n",
      "step: 30700, loss: 2.5042e-03, time:   248\n",
      "step: 30800, loss: 3.7599e-02, time:   249\n",
      "step: 30900, loss: 1.1805e-03, time:   250\n",
      "step: 31000, loss: 1.1465e-02, time:   251\n",
      "step: 31100, loss: 1.1534e-03, time:   252\n",
      "step: 31200, loss: 1.5724e-02, time:   253\n",
      "step: 31300, loss: 2.2844e-03, time:   254\n",
      "step: 31400, loss: 1.4806e-01, time:   254\n",
      "step: 31500, loss: 3.2503e-03, time:   255\n",
      "step: 31600, loss: 1.0775e-03, time:   256\n",
      "step: 31700, loss: 9.6976e-04, time:   257\n",
      "step: 31800, loss: 9.9332e-04, time:   258\n",
      "step: 31900, loss: 3.5387e-03, time:   259\n",
      "step: 32000, loss: 1.3231e-02, time:   259\n",
      "step: 32100, loss: 3.1160e-02, time:   260\n",
      "step: 32200, loss: 1.1355e-02, time:   261\n",
      "step: 32300, loss: 9.8030e-04, time:   262\n",
      "step: 32400, loss: 3.5071e-03, time:   263\n",
      "step: 32500, loss: 3.4809e-03, time:   263\n",
      "step: 32600, loss: 9.0668e-04, time:   264\n",
      "step: 32700, loss: 1.4884e-01, time:   265\n",
      "step: 32800, loss: 1.0172e-02, time:   266\n",
      "step: 32900, loss: 1.2469e-03, time:   267\n",
      "step: 33000, loss: 4.2056e-03, time:   267\n",
      "step: 33100, loss: 1.9816e-03, time:   268\n",
      "step: 33200, loss: 1.3653e-03, time:   269\n",
      "step: 33300, loss: 8.5125e-02, time:   270\n",
      "step: 33400, loss: 8.5741e-04, time:   271\n",
      "step: 33500, loss: 5.8970e-03, time:   271\n",
      "step: 33600, loss: 7.6905e-02, time:   272\n",
      "step: 33700, loss: 1.0946e-03, time:   273\n",
      "step: 33800, loss: 1.1233e-02, time:   274\n",
      "step: 33900, loss: 2.7263e-02, time:   274\n",
      "step: 34000, loss: 8.6153e-03, time:   275\n",
      "step: 34100, loss: 2.3819e-03, time:   276\n",
      "step: 34200, loss: 6.2064e-02, time:   277\n",
      "step: 34300, loss: 1.9865e-03, time:   278\n",
      "step: 34400, loss: 5.9860e-03, time:   278\n",
      "step: 34500, loss: 1.1362e-03, time:   279\n",
      "step: 34600, loss: 3.9308e-03, time:   280\n",
      "step: 34700, loss: 1.2121e-03, time:   281\n",
      "step: 34800, loss: 1.2956e-02, time:   282\n",
      "step: 34900, loss: 2.1927e-02, time:   283\n",
      "step: 35000, loss: 5.3346e-03, time:   283\n",
      "step: 35100, loss: 4.7228e-03, time:   284\n",
      "step: 35200, loss: 1.0858e-03, time:   285\n",
      "step: 35300, loss: 9.1717e-04, time:   286\n",
      "step: 35400, loss: 8.5062e-04, time:   287\n",
      "step: 35500, loss: 1.0481e-03, time:   288\n",
      "step: 35600, loss: 1.2610e-03, time:   288\n",
      "step: 35700, loss: 2.8789e-03, time:   289\n",
      "step: 35800, loss: 1.1875e-03, time:   290\n",
      "step: 35900, loss: 1.0139e-03, time:   291\n",
      "step: 36000, loss: 1.0123e-02, time:   292\n",
      "step: 36100, loss: 1.5454e-03, time:   293\n",
      "step: 36200, loss: 9.4677e-04, time:   293\n",
      "step: 36300, loss: 3.3623e-02, time:   294\n",
      "step: 36400, loss: 6.7194e-03, time:   295\n",
      "step: 36500, loss: 1.4765e-03, time:   296\n",
      "step: 36600, loss: 7.7333e-04, time:   297\n",
      "step: 36700, loss: 3.0069e-02, time:   298\n",
      "step: 36800, loss: 9.2583e-04, time:   298\n",
      "step: 36900, loss: 1.8682e-01, time:   299\n",
      "step: 37000, loss: 3.6150e-03, time:   300\n",
      "step: 37100, loss: 9.3261e-04, time:   301\n",
      "step: 37200, loss: 2.9295e-01, time:   302\n",
      "step: 37300, loss: 3.0650e-03, time:   302\n",
      "step: 37400, loss: 8.0097e-04, time:   303\n",
      "step: 37500, loss: 1.6315e-01, time:   304\n",
      "step: 37600, loss: 9.2960e-04, time:   305\n",
      "step: 37700, loss: 1.0539e-03, time:   306\n",
      "step: 37800, loss: 1.1011e-01, time:   306\n",
      "step: 37900, loss: 1.2936e-03, time:   307\n",
      "step: 38000, loss: 7.5164e-04, time:   308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38100, loss: 1.4087e-03, time:   309\n",
      "step: 38200, loss: 9.5053e-02, time:   310\n",
      "step: 38300, loss: 1.0676e-03, time:   310\n",
      "step: 38400, loss: 9.4000e-04, time:   311\n",
      "step: 38500, loss: 1.8359e-03, time:   312\n",
      "step: 38600, loss: 4.3205e-02, time:   313\n",
      "step: 38700, loss: 4.6599e-03, time:   313\n",
      "step: 38800, loss: 1.2248e-03, time:   314\n",
      "step: 38900, loss: 1.2333e-03, time:   315\n",
      "step: 39000, loss: 9.6020e-04, time:   316\n",
      "step: 39100, loss: 4.8598e-02, time:   317\n",
      "step: 39200, loss: 4.6758e-02, time:   317\n",
      "step: 39300, loss: 1.1585e-03, time:   318\n",
      "step: 39400, loss: 6.6246e-04, time:   319\n",
      "step: 39500, loss: 5.3914e-04, time:   320\n",
      "step: 39600, loss: 4.7608e-04, time:   321\n",
      "[10.99362494  0.03459251  0.         19.99030746]\n"
     ]
    }
   ],
   "source": [
    "class framework(object):\n",
    "    \"\"\"The fully - connected neural network model.\"\"\"\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "    # parameters for the function we want to approximate\n",
    "        self.domain_bound = 150   # boundary value M\n",
    "        self.terminal_time = 1\n",
    "        \n",
    "    # parameters for the algorithm\n",
    "        self.n_simple_train1 = 425 # number samples in training set \n",
    "        self.n_simple_train2 = 300\n",
    "        self.n_simple_train3 = 300\n",
    "        self.n_simple_valid = 2000 # number samples in validation set\n",
    "\n",
    "        self.hidden_neuron = 50  #number of neurons in each layer \n",
    "        self.n_neuron = [1, self.hidden_neuron, self.hidden_neuron, 1] \n",
    "        #structure of of neural net\n",
    "        \n",
    "    \n",
    "        self.n_maxstep = 200000 # number of maximum iteration\n",
    "        self.n_displaystep = 100 # every n_displaystep steps, we output some information\n",
    "        self.picdisplaystep= 200 # every picdisplaystep steps, we output graph\n",
    "        self.learning_rate = 5e-3\n",
    "        \n",
    "        # some basic constants and variables\n",
    "        self._extra_train_ops = []\n",
    "        self.tolerence = 5e-4\n",
    "        \n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.loss, trainable_vars)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        apply_op = \\\n",
    "                optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        train_ops = [apply_op] + self._extra_train_ops\n",
    "        self.train_op = tf.group(* train_ops)\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.time_history = []\n",
    "        \n",
    "#=============================================================================\n",
    "# You can change the code in this block if needed. But make sure always feed \n",
    "# neural net what it needs. \n",
    "        x_valid1 = self.sample_point_X1(self.n_simple_valid)\n",
    "        x_valid2 = self.sample_point_X2(self.n_simple_valid)\n",
    "        x_valid3 = self.sample_point_X3(self.n_simple_valid)\n",
    "        feed_dict_valid = {self.Xi: x_valid1, self.Xb: x_valid2, self.Xt: x_valid3, self.is_training: False}\n",
    "#=============================================================================\n",
    "        print(\"before initialized\")\n",
    "        \n",
    "        # initialization\n",
    "        step = 1\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        temp_loss = self.sess.run(self.loss,\n",
    "                                  feed_dict = feed_dict_valid)\n",
    "        self.loss_history.append(temp_loss)\n",
    "        self.time_history.append(0)\n",
    "        \n",
    "        print(\"step: %5u, loss: %.4e,\" % \\\n",
    "              (0, temp_loss))\n",
    "        \n",
    "        #x_test_gradient = np.array([[0],[1],[2],[3]])\n",
    "        #gradient_test = self.sess.run(self.gradient_test,\n",
    "                                     #feed_dict = {self.X: x_test_gradient,\n",
    "                                                 #self.is_training: False})\n",
    "        \n",
    "        # begin sgd iteration\n",
    "        # for iii in range (self.n_maxstep + 2):\n",
    "        while (self.loss_history[-1] > self.tolerence) and (step < self.n_maxstep) :\n",
    "             x_train1 = self.sample_point_X1(self.n_simple_train1)\n",
    "             x_train2 = self.sample_point_X2(self.n_simple_train2)\n",
    "             x_train3 = self.sample_point_X3(self.n_simple_train3)\n",
    "             self.sess.run(self.train_op,\n",
    "                           feed_dict = {self.Xi: x_train1, self.Xb: x_train2, self.Xt: x_train3,\n",
    "                                       self.is_training: True})\n",
    "             if step % self.n_displaystep == 0:\n",
    "                 temp_loss = self.sess.run(self.loss,\n",
    "                                           feed_dict = feed_dict_valid)\n",
    "                 self.loss_history.append(temp_loss)\n",
    "                 self.time_history.append(time.time() - start_time + self.t_bd)\n",
    "                 print (\"step: %5u, loss: %.4e, time: %5u\" % \\\n",
    "                        (step , temp_loss, time.time() - start_time))\n",
    "\n",
    "             #if step % self .picdisplaystep == 0:\n",
    "                     #approximate_f = self.sess.run(self.output,\n",
    "                                                   #feed_dict = feed_dict_valid)                     \n",
    "                     #plt.scatter(x_valid, approximate_f, label = 'approximation of neural net')\n",
    "                     #plt.scatter(x_valid, self.f(x_valid), label = 'real function f')\n",
    "                     #plt.legend()\n",
    "                     #plt.show()\n",
    "             step += 1\n",
    "\n",
    "        x1 = [[0, 100], [1, 90], [1, 80], [1, 110]]\n",
    "        self.outputsample = self.sess.run(self.output, \n",
    "                                          feed_dict = {self.Xi: x1, self.Xb: x_valid2, \n",
    "                                                       self.Xt: x_valid3, self.is_training: False})\n",
    "#=============================================================================        \n",
    "# You will need to rewrite this block. \n",
    "    def build(self):\n",
    "        start_time = time.time()\n",
    "        self.Xi = tf.placeholder(tf.float64, [None, 2], name = 'Xi') # interior\n",
    "        self.Xb = tf.placeholder(tf.float64, [None, 2], name = 'Xb') # boundary\n",
    "        self.Xt = tf.placeholder(tf.float64, [None, 2], name = 'Xt') # terminal\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        \n",
    "# the variable f_ is output of neural net given certain input.       \n",
    "        c_interior = self._one_time_net(self.Xi, 'c')[:,0]\n",
    "        c_boundary = self._one_time_net(self.Xb, 'c')[:,0]\n",
    "        c_terminal = self._one_time_net(self.Xt, 'c')[:,0]\n",
    "        \n",
    "# loss function here is just L2 norm of difference of this two function.\n",
    "        c_interior_dt = tf.gradients(c_interior, self.Xi)[0][:,0]\n",
    "        c_interior_dx = tf.gradients(c_interior, self.Xi)[0][:,1]\n",
    "        c_interior_dxx = tf.gradients(c_interior_dx, self.Xi)[0][:,1]\n",
    "        c_boundary_dx = tf.gradients(c_boundary, self.Xb)[0][:,1]\n",
    "        \n",
    "        loss_terminal = c_terminal - tf.math.maximum(self.Xt[:,1]-90, 0)\n",
    "        loss_boundary = c_boundary_dx - 1\n",
    "        loss_interior = c_interior_dt + 0.01*tf.math.multiply(self.Xi[:,1], c_interior_dx) \n",
    "        + 0.5*0.04*tf.math.multiply(tf.math.square(self.Xi[:,1]), c_interior_dxx) - 0.01*c_interior\n",
    "    \n",
    "        self.loss = tf.reduce_mean((loss_terminal)**2) + tf.reduce_mean((loss_boundary)**2) + tf.reduce_mean((loss_interior)**2)\n",
    "        \n",
    "# test of tf.gradient function\n",
    "        #self.gradient_test = tf.gradients(self.f(self.X), self.X)\n",
    "#=============================================================================  \n",
    "        self.output = c_interior\n",
    "        self.t_bd = time.time() - start_time        \n",
    "        print('end of build')\n",
    "#=============================================================================        \n",
    "# this function define how we sample points in domain\n",
    "    def sample_point_X1(self, n_sample1): #interior\n",
    "        t = np.random.uniform(low = 0, high = self.terminal_time, size = (n_sample1, 1))\n",
    "        x = np.random.uniform(low = 0, high = self.domain_bound, size = (n_sample1, 1))\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "    def sample_point_X2(self, n_sample2): # boundary x=M\n",
    "        t = np.random.uniform(low = 0, high = self.terminal_time, size = (n_sample2, 1))\n",
    "        x = np.ones((n_sample2, 1)) * self.domain_bound\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "    def sample_point_X3(self, n_sample3): # terminal t=T\n",
    "        t = np.ones((n_sample3, 1)) * self.terminal_time\n",
    "        x = np.random.uniform(low = 0, high = self.domain_bound, size = (n_sample3, 1))\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "# this is the function we want to approximate\n",
    "#    def f(self, x):\n",
    "#        return x**3\n",
    "    \n",
    "# you can change number of layer or activation function if needed\n",
    "    def _one_time_net(self, x, name):\n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            layer1 = self._one_layer(x, self.n_neuron[1], activation_fn = tf.nn.elu, name = 'layer1')\n",
    "            layer2 = self._one_layer(layer1, self.n_neuron[2], activation_fn = tf.nn.relu,  name = 'layer2')\n",
    "            layer3 = self._one_layer(layer2, self.n_neuron[2], activation_fn = tf.nn.relu,  name = 'layer2')\n",
    "            u = self._one_layer(layer3, self.n_neuron[3], activation_fn = None ,name ='final')\n",
    "        return u\n",
    "\n",
    "# you can change initialization of weight in each layer if needed\n",
    "    def _one_layer(self, input_, out_sz,\n",
    "                   activation_fn = None,\n",
    "                   std =1.0, name = 'linear'):\n",
    "        with tf.variable_scope(name,):\n",
    "            shape = input_.get_shape().as_list()\n",
    "            w = tf.get_variable('Matrix',\n",
    "                                [shape[1], out_sz], tf.float64,\n",
    "                                norm_init(stddev = \\\n",
    "                                          std / np.sqrt(shape[1] + out_sz)))\n",
    "            hidden = tf.matmul(input_, w)\n",
    "        if activation_fn != None:\n",
    "            return activation_fn(hidden)\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "#if __name__ == '__main__ ':\n",
    "#    np.random.seed(1)\n",
    "#    main()\n",
    "#main()\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.set_random_seed(1)\n",
    "    print(\"begin to solve approximation\")\n",
    "    model = framework(sess)\n",
    "    model.build()\n",
    "    model.train()\n",
    "    output = np.zeros((len(model.loss_history), 3))\n",
    "    output[:,0] = np.arange(len(model.loss_history)) \\\n",
    "    * model.n_displaystep\n",
    "    output[:,1] = model.loss_history\n",
    "    output[:,2] = model.time_history\n",
    "    outputsample = model.outputsample\n",
    "    print(outputsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Convergence rate (time)')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x = [500,475 ,450 ,425, 400 ,350 ,325, 300]\n",
    "y = [473 ,377, 334, 257, 368 ,391 ,455, 494]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Number of data points\")\n",
    "plt.ylabel(\"Convergence rate (time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}, \\quad u = u(t,x), \\quad x\\in[0,M],\\\\\n",
    "u(0,x) = \\sin(2\\pi x),\\\\\n",
    "u(t,0) = u(t,M) = 1,\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to solve approximation\n",
      "end of build\n",
      "before initialized\n",
      "step:     0, loss: 1.9640e+00,\n",
      "step:   100, loss: 1.0553e+00, time:     1\n",
      "step:   200, loss: 9.3654e-01, time:     1\n",
      "step:   300, loss: 9.2003e-01, time:     1\n",
      "step:   400, loss: 8.8856e-01, time:     1\n",
      "step:   500, loss: 8.7418e-01, time:     2\n",
      "step:   600, loss: 8.5816e-01, time:     2\n",
      "step:   700, loss: 8.5571e-01, time:     2\n",
      "step:   800, loss: 8.4745e-01, time:     3\n",
      "step:   900, loss: 8.3332e-01, time:     3\n",
      "step:  1000, loss: 8.3880e-01, time:     3\n",
      "step:  1100, loss: 8.1863e-01, time:     3\n",
      "step:  1200, loss: 8.2068e-01, time:     4\n",
      "step:  1300, loss: 8.0589e-01, time:     4\n",
      "step:  1400, loss: 8.0151e-01, time:     4\n",
      "step:  1500, loss: 7.8357e-01, time:     4\n",
      "step:  1600, loss: 7.6554e-01, time:     5\n",
      "step:  1700, loss: 7.3539e-01, time:     5\n",
      "step:  1800, loss: 7.1213e-01, time:     5\n",
      "step:  1900, loss: 6.9182e-01, time:     5\n",
      "step:  2000, loss: 6.7544e-01, time:     6\n",
      "step:  2100, loss: 6.6620e-01, time:     6\n",
      "step:  2200, loss: 6.6948e-01, time:     6\n",
      "step:  2300, loss: 6.6110e-01, time:     6\n",
      "step:  2400, loss: 6.5229e-01, time:     7\n",
      "step:  2500, loss: 6.4261e-01, time:     7\n",
      "step:  2600, loss: 6.3658e-01, time:     7\n",
      "step:  2700, loss: 6.3122e-01, time:     8\n",
      "step:  2800, loss: 6.3356e-01, time:     8\n",
      "step:  2900, loss: 6.3618e-01, time:     8\n",
      "step:  3000, loss: 6.2564e-01, time:     8\n",
      "step:  3100, loss: 6.2638e-01, time:     9\n",
      "step:  3200, loss: 6.2124e-01, time:     9\n",
      "step:  3300, loss: 6.2566e-01, time:     9\n",
      "step:  3400, loss: 6.2066e-01, time:     9\n",
      "step:  3500, loss: 6.4822e-01, time:    10\n",
      "step:  3600, loss: 6.3234e-01, time:    10\n",
      "step:  3700, loss: 6.1075e-01, time:    10\n",
      "step:  3800, loss: 6.1485e-01, time:    10\n",
      "step:  3900, loss: 6.1394e-01, time:    11\n",
      "step:  4000, loss: 6.0756e-01, time:    11\n",
      "step:  4100, loss: 6.1302e-01, time:    11\n",
      "step:  4200, loss: 6.1828e-01, time:    12\n",
      "step:  4300, loss: 6.0264e-01, time:    12\n",
      "step:  4400, loss: 6.0293e-01, time:    12\n",
      "step:  4500, loss: 6.1010e-01, time:    12\n",
      "step:  4600, loss: 6.0264e-01, time:    13\n",
      "step:  4700, loss: 6.0416e-01, time:    13\n",
      "step:  4800, loss: 5.9936e-01, time:    13\n",
      "step:  4900, loss: 5.9502e-01, time:    14\n",
      "step:  5000, loss: 5.9635e-01, time:    14\n",
      "step:  5100, loss: 5.9803e-01, time:    14\n",
      "step:  5200, loss: 5.9622e-01, time:    14\n",
      "step:  5300, loss: 5.9140e-01, time:    15\n",
      "step:  5400, loss: 6.0360e-01, time:    15\n",
      "step:  5500, loss: 5.9238e-01, time:    15\n",
      "step:  5600, loss: 5.8956e-01, time:    16\n",
      "step:  5700, loss: 5.9463e-01, time:    16\n",
      "step:  5800, loss: 5.8959e-01, time:    16\n",
      "step:  5900, loss: 5.9221e-01, time:    16\n",
      "step:  6000, loss: 5.8623e-01, time:    17\n",
      "step:  6100, loss: 5.9845e-01, time:    17\n",
      "step:  6200, loss: 5.9707e-01, time:    17\n",
      "step:  6300, loss: 5.9153e-01, time:    17\n",
      "step:  6400, loss: 5.8146e-01, time:    18\n",
      "step:  6500, loss: 5.8388e-01, time:    18\n",
      "step:  6600, loss: 5.8487e-01, time:    18\n",
      "step:  6700, loss: 5.8009e-01, time:    19\n",
      "step:  6800, loss: 5.7596e-01, time:    19\n",
      "step:  6900, loss: 5.7775e-01, time:    19\n",
      "step:  7000, loss: 5.8786e-01, time:    20\n",
      "step:  7100, loss: 5.7463e-01, time:    20\n",
      "step:  7200, loss: 5.7242e-01, time:    20\n",
      "step:  7300, loss: 5.8182e-01, time:    20\n",
      "step:  7400, loss: 5.7836e-01, time:    21\n",
      "step:  7500, loss: 5.9093e-01, time:    21\n",
      "step:  7600, loss: 5.7346e-01, time:    21\n",
      "step:  7700, loss: 5.7663e-01, time:    22\n",
      "step:  7800, loss: 5.7690e-01, time:    22\n",
      "step:  7900, loss: 5.7532e-01, time:    22\n",
      "step:  8000, loss: 5.7194e-01, time:    22\n",
      "step:  8100, loss: 5.7554e-01, time:    22\n",
      "step:  8200, loss: 5.6957e-01, time:    23\n",
      "step:  8300, loss: 5.7660e-01, time:    23\n",
      "step:  8400, loss: 5.7011e-01, time:    23\n",
      "step:  8500, loss: 5.6903e-01, time:    23\n",
      "step:  8600, loss: 5.7179e-01, time:    24\n",
      "step:  8700, loss: 5.8368e-01, time:    24\n",
      "step:  8800, loss: 5.6882e-01, time:    24\n",
      "step:  8900, loss: 5.6736e-01, time:    24\n",
      "step:  9000, loss: 5.7478e-01, time:    25\n",
      "step:  9100, loss: 5.6536e-01, time:    25\n",
      "step:  9200, loss: 5.6764e-01, time:    25\n",
      "step:  9300, loss: 5.6635e-01, time:    26\n",
      "step:  9400, loss: 5.7124e-01, time:    26\n",
      "step:  9500, loss: 5.6449e-01, time:    26\n",
      "step:  9600, loss: 5.6543e-01, time:    26\n",
      "step:  9700, loss: 5.6710e-01, time:    27\n",
      "step:  9800, loss: 5.8772e-01, time:    27\n",
      "step:  9900, loss: 5.7096e-01, time:    27\n",
      "step: 10000, loss: 5.6861e-01, time:    28\n",
      "step: 10100, loss: 5.7389e-01, time:    28\n",
      "step: 10200, loss: 5.6227e-01, time:    28\n",
      "step: 10300, loss: 5.6164e-01, time:    28\n",
      "step: 10400, loss: 5.7073e-01, time:    29\n",
      "step: 10500, loss: 5.6533e-01, time:    29\n",
      "step: 10600, loss: 5.6902e-01, time:    29\n",
      "step: 10700, loss: 5.9682e-01, time:    30\n",
      "step: 10800, loss: 5.7098e-01, time:    30\n",
      "step: 10900, loss: 5.6883e-01, time:    30\n",
      "step: 11000, loss: 5.9047e-01, time:    30\n",
      "step: 11100, loss: 5.6449e-01, time:    31\n",
      "step: 11200, loss: 5.6984e-01, time:    31\n",
      "step: 11300, loss: 5.6060e-01, time:    31\n",
      "step: 11400, loss: 5.6260e-01, time:    32\n",
      "step: 11500, loss: 5.7584e-01, time:    32\n",
      "step: 11600, loss: 5.5900e-01, time:    32\n",
      "step: 11700, loss: 5.6107e-01, time:    33\n",
      "step: 11800, loss: 5.6078e-01, time:    33\n",
      "step: 11900, loss: 5.8504e-01, time:    33\n",
      "step: 12000, loss: 5.6865e-01, time:    34\n",
      "step: 12100, loss: 5.6486e-01, time:    34\n",
      "step: 12200, loss: 5.6407e-01, time:    35\n",
      "step: 12300, loss: 5.6210e-01, time:    35\n",
      "step: 12400, loss: 5.6185e-01, time:    35\n",
      "step: 12500, loss: 5.5886e-01, time:    36\n",
      "step: 12600, loss: 5.6050e-01, time:    36\n",
      "step: 12700, loss: 5.6557e-01, time:    37\n",
      "step: 12800, loss: 5.6264e-01, time:    37\n",
      "step: 12900, loss: 5.6347e-01, time:    37\n",
      "step: 13000, loss: 5.7221e-01, time:    38\n",
      "step: 13100, loss: 5.6588e-01, time:    38\n",
      "step: 13200, loss: 5.7110e-01, time:    38\n",
      "step: 13300, loss: 5.6202e-01, time:    39\n",
      "step: 13400, loss: 5.5730e-01, time:    39\n",
      "step: 13500, loss: 5.6049e-01, time:    40\n",
      "step: 13600, loss: 5.6716e-01, time:    40\n",
      "step: 13700, loss: 5.6012e-01, time:    40\n",
      "step: 13800, loss: 5.5922e-01, time:    41\n",
      "step: 13900, loss: 5.5534e-01, time:    41\n",
      "step: 14000, loss: 5.6105e-01, time:    42\n",
      "step: 14100, loss: 5.5866e-01, time:    42\n",
      "step: 14200, loss: 5.6305e-01, time:    42\n",
      "step: 14300, loss: 5.6776e-01, time:    43\n",
      "step: 14400, loss: 5.6636e-01, time:    43\n",
      "step: 14500, loss: 5.6906e-01, time:    44\n",
      "step: 14600, loss: 5.6627e-01, time:    44\n",
      "step: 14700, loss: 5.5859e-01, time:    44\n",
      "step: 14800, loss: 5.6190e-01, time:    45\n",
      "step: 14900, loss: 5.6142e-01, time:    45\n",
      "step: 15000, loss: 5.5976e-01, time:    45\n",
      "step: 15100, loss: 5.6611e-01, time:    46\n",
      "step: 15200, loss: 5.6007e-01, time:    46\n",
      "step: 15300, loss: 5.5779e-01, time:    46\n",
      "step: 15400, loss: 5.8452e-01, time:    47\n",
      "step: 15500, loss: 5.6959e-01, time:    47\n",
      "step: 15600, loss: 5.8249e-01, time:    48\n",
      "step: 15700, loss: 5.6446e-01, time:    48\n",
      "step: 15800, loss: 5.5987e-01, time:    48\n",
      "step: 15900, loss: 5.6423e-01, time:    49\n",
      "step: 16000, loss: 5.6052e-01, time:    49\n",
      "step: 16100, loss: 5.5952e-01, time:    50\n",
      "step: 16200, loss: 5.7037e-01, time:    50\n",
      "step: 16300, loss: 5.7656e-01, time:    50\n",
      "step: 16400, loss: 5.5957e-01, time:    51\n",
      "step: 16500, loss: 5.6293e-01, time:    51\n",
      "step: 16600, loss: 5.7157e-01, time:    51\n",
      "step: 16700, loss: 5.6514e-01, time:    52\n",
      "step: 16800, loss: 5.7087e-01, time:    52\n",
      "step: 16900, loss: 5.5893e-01, time:    52\n",
      "step: 17000, loss: 5.6585e-01, time:    52\n",
      "step: 17100, loss: 5.6410e-01, time:    53\n",
      "step: 17200, loss: 5.6306e-01, time:    53\n",
      "step: 17300, loss: 5.6264e-01, time:    53\n",
      "step: 17400, loss: 5.8127e-01, time:    54\n",
      "step: 17500, loss: 5.6021e-01, time:    54\n",
      "step: 17600, loss: 5.6410e-01, time:    54\n",
      "step: 17700, loss: 5.5886e-01, time:    55\n",
      "step: 17800, loss: 5.8897e-01, time:    55\n",
      "step: 17900, loss: 5.6895e-01, time:    55\n",
      "step: 18000, loss: 5.6075e-01, time:    55\n",
      "step: 18100, loss: 5.6162e-01, time:    56\n",
      "step: 18200, loss: 5.6556e-01, time:    56\n",
      "step: 18300, loss: 5.6292e-01, time:    56\n",
      "step: 18400, loss: 5.6317e-01, time:    56\n",
      "step: 18500, loss: 5.6524e-01, time:    57\n",
      "step: 18600, loss: 5.6246e-01, time:    57\n",
      "step: 18700, loss: 5.6997e-01, time:    58\n",
      "step: 18800, loss: 5.6863e-01, time:    58\n",
      "step: 18900, loss: 5.6220e-01, time:    58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19000, loss: 5.6204e-01, time:    59\n",
      "step: 19100, loss: 5.7892e-01, time:    59\n",
      "step: 19200, loss: 5.6213e-01, time:    59\n",
      "step: 19300, loss: 5.6328e-01, time:    60\n",
      "step: 19400, loss: 5.6281e-01, time:    60\n",
      "step: 19500, loss: 5.6128e-01, time:    60\n",
      "step: 19600, loss: 5.6736e-01, time:    61\n",
      "step: 19700, loss: 5.6033e-01, time:    61\n",
      "step: 19800, loss: 5.6551e-01, time:    61\n",
      "step: 19900, loss: 5.7086e-01, time:    62\n",
      "step: 20000, loss: 5.8074e-01, time:    62\n",
      "step: 20100, loss: 5.6834e-01, time:    63\n",
      "step: 20200, loss: 5.6404e-01, time:    63\n",
      "step: 20300, loss: 5.7395e-01, time:    63\n",
      "step: 20400, loss: 5.6946e-01, time:    64\n",
      "step: 20500, loss: 5.6961e-01, time:    64\n",
      "step: 20600, loss: 5.6490e-01, time:    65\n",
      "step: 20700, loss: 5.6514e-01, time:    65\n",
      "step: 20800, loss: 5.6616e-01, time:    65\n",
      "step: 20900, loss: 5.6773e-01, time:    66\n",
      "step: 21000, loss: 5.7585e-01, time:    66\n",
      "step: 21100, loss: 5.7806e-01, time:    67\n",
      "step: 21200, loss: 5.6551e-01, time:    67\n",
      "step: 21300, loss: 5.5814e-01, time:    68\n",
      "step: 21400, loss: 5.5673e-01, time:    68\n",
      "step: 21500, loss: 5.5665e-01, time:    68\n",
      "step: 21600, loss: 5.6536e-01, time:    69\n",
      "step: 21700, loss: 5.5778e-01, time:    69\n",
      "step: 21800, loss: 5.5541e-01, time:    70\n",
      "step: 21900, loss: 5.5904e-01, time:    70\n",
      "step: 22000, loss: 5.6214e-01, time:    70\n",
      "step: 22100, loss: 5.7432e-01, time:    71\n",
      "step: 22200, loss: 5.6163e-01, time:    71\n",
      "step: 22300, loss: 5.7809e-01, time:    72\n",
      "step: 22400, loss: 5.5795e-01, time:    72\n",
      "step: 22500, loss: 5.6122e-01, time:    72\n",
      "step: 22600, loss: 5.5722e-01, time:    73\n",
      "step: 22700, loss: 5.6229e-01, time:    73\n",
      "step: 22800, loss: 5.5502e-01, time:    74\n",
      "step: 22900, loss: 5.6119e-01, time:    74\n",
      "step: 23000, loss: 5.5579e-01, time:    74\n",
      "step: 23100, loss: 5.5264e-01, time:    75\n",
      "step: 23200, loss: 5.5292e-01, time:    75\n",
      "step: 23300, loss: 5.5389e-01, time:    76\n",
      "step: 23400, loss: 5.5284e-01, time:    76\n",
      "step: 23500, loss: 5.5774e-01, time:    76\n",
      "step: 23600, loss: 5.6203e-01, time:    77\n",
      "step: 23700, loss: 5.5810e-01, time:    77\n",
      "step: 23800, loss: 5.5214e-01, time:    78\n",
      "step: 23900, loss: 5.6394e-01, time:    78\n",
      "step: 24000, loss: 5.6257e-01, time:    79\n",
      "step: 24100, loss: 5.6097e-01, time:    79\n",
      "step: 24200, loss: 5.5766e-01, time:    79\n",
      "step: 24300, loss: 5.7448e-01, time:    80\n",
      "step: 24400, loss: 5.5591e-01, time:    80\n",
      "step: 24500, loss: 5.5546e-01, time:    81\n",
      "step: 24600, loss: 5.5792e-01, time:    81\n",
      "step: 24700, loss: 5.5474e-01, time:    81\n",
      "step: 24800, loss: 5.5641e-01, time:    82\n",
      "step: 24900, loss: 5.5203e-01, time:    82\n",
      "step: 25000, loss: 5.5921e-01, time:    83\n",
      "step: 25100, loss: 5.5269e-01, time:    83\n",
      "step: 25200, loss: 5.5124e-01, time:    83\n",
      "step: 25300, loss: 5.5458e-01, time:    84\n",
      "step: 25400, loss: 5.5413e-01, time:    84\n",
      "step: 25500, loss: 5.5269e-01, time:    85\n",
      "step: 25600, loss: 5.5399e-01, time:    85\n",
      "step: 25700, loss: 5.5770e-01, time:    85\n",
      "step: 25800, loss: 5.5475e-01, time:    86\n",
      "step: 25900, loss: 5.5562e-01, time:    86\n",
      "step: 26000, loss: 5.6008e-01, time:    87\n",
      "step: 26100, loss: 5.5140e-01, time:    87\n",
      "step: 26200, loss: 5.5469e-01, time:    87\n",
      "step: 26300, loss: 5.5330e-01, time:    88\n",
      "step: 26400, loss: 5.5091e-01, time:    88\n",
      "step: 26500, loss: 5.5379e-01, time:    89\n",
      "step: 26600, loss: 5.5960e-01, time:    89\n",
      "step: 26700, loss: 5.6343e-01, time:    90\n",
      "step: 26800, loss: 5.5646e-01, time:    90\n",
      "step: 26900, loss: 5.4898e-01, time:    90\n",
      "step: 27000, loss: 5.4998e-01, time:    91\n",
      "step: 27100, loss: 5.4936e-01, time:    91\n",
      "step: 27200, loss: 5.5022e-01, time:    92\n",
      "step: 27300, loss: 5.5034e-01, time:    92\n",
      "step: 27400, loss: 5.6974e-01, time:    92\n",
      "step: 27500, loss: 5.5470e-01, time:    93\n",
      "step: 27600, loss: 5.5491e-01, time:    93\n",
      "step: 27700, loss: 5.9065e-01, time:    94\n",
      "step: 27800, loss: 5.6046e-01, time:    94\n",
      "step: 27900, loss: 5.5157e-01, time:    94\n",
      "step: 28000, loss: 5.7259e-01, time:    95\n",
      "step: 28100, loss: 5.5392e-01, time:    95\n",
      "step: 28200, loss: 5.5340e-01, time:    96\n",
      "step: 28300, loss: 5.5183e-01, time:    96\n",
      "step: 28400, loss: 5.5237e-01, time:    96\n",
      "step: 28500, loss: 5.5083e-01, time:    97\n",
      "step: 28600, loss: 5.4916e-01, time:    97\n",
      "step: 28700, loss: 6.0951e-01, time:    98\n",
      "step: 28800, loss: 5.5169e-01, time:    98\n",
      "step: 28900, loss: 5.4976e-01, time:    98\n",
      "step: 29000, loss: 5.5407e-01, time:    99\n",
      "step: 29100, loss: 5.5123e-01, time:    99\n",
      "step: 29200, loss: 5.6387e-01, time:   100\n",
      "step: 29300, loss: 5.6518e-01, time:   100\n",
      "step: 29400, loss: 5.5426e-01, time:   100\n",
      "step: 29500, loss: 5.5253e-01, time:   101\n",
      "step: 29600, loss: 5.6989e-01, time:   101\n",
      "step: 29700, loss: 5.5348e-01, time:   102\n",
      "step: 29800, loss: 5.5151e-01, time:   102\n",
      "step: 29900, loss: 5.5908e-01, time:   103\n",
      "step: 30000, loss: 5.5130e-01, time:   103\n",
      "step: 30100, loss: 5.5203e-01, time:   103\n",
      "step: 30200, loss: 5.4862e-01, time:   104\n",
      "step: 30300, loss: 5.5202e-01, time:   104\n",
      "step: 30400, loss: 5.4759e-01, time:   105\n",
      "step: 30500, loss: 5.5038e-01, time:   105\n",
      "step: 30600, loss: 5.5297e-01, time:   106\n",
      "step: 30700, loss: 5.4827e-01, time:   106\n",
      "step: 30800, loss: 5.6379e-01, time:   106\n",
      "step: 30900, loss: 5.5378e-01, time:   107\n",
      "step: 31000, loss: 5.5542e-01, time:   107\n",
      "step: 31100, loss: 5.4621e-01, time:   108\n",
      "step: 31200, loss: 5.4902e-01, time:   108\n",
      "step: 31300, loss: 5.5152e-01, time:   109\n",
      "step: 31400, loss: 5.4404e-01, time:   109\n",
      "step: 31500, loss: 5.5260e-01, time:   110\n",
      "step: 31600, loss: 5.4742e-01, time:   110\n",
      "step: 31700, loss: 5.4796e-01, time:   111\n",
      "step: 31800, loss: 5.4824e-01, time:   111\n",
      "step: 31900, loss: 5.5573e-01, time:   111\n",
      "step: 32000, loss: 5.5427e-01, time:   112\n",
      "step: 32100, loss: 5.4903e-01, time:   112\n",
      "step: 32200, loss: 5.4990e-01, time:   113\n",
      "step: 32300, loss: 5.5552e-01, time:   113\n",
      "step: 32400, loss: 5.4000e-01, time:   114\n",
      "step: 32500, loss: 5.5177e-01, time:   114\n",
      "step: 32600, loss: 5.5217e-01, time:   114\n",
      "step: 32700, loss: 5.5454e-01, time:   115\n",
      "step: 32800, loss: 5.6781e-01, time:   115\n",
      "step: 32900, loss: 5.5490e-01, time:   116\n",
      "step: 33000, loss: 5.6248e-01, time:   116\n",
      "step: 33100, loss: 5.4151e-01, time:   117\n",
      "step: 33200, loss: 5.7040e-01, time:   117\n",
      "step: 33300, loss: 5.6238e-01, time:   117\n",
      "step: 33400, loss: 5.4318e-01, time:   118\n",
      "step: 33500, loss: 5.4389e-01, time:   118\n",
      "step: 33600, loss: 5.3541e-01, time:   119\n",
      "step: 33700, loss: 5.4720e-01, time:   119\n",
      "step: 33800, loss: 5.3549e-01, time:   119\n",
      "step: 33900, loss: 5.3590e-01, time:   120\n",
      "step: 34000, loss: 5.3812e-01, time:   120\n",
      "step: 34100, loss: 5.3597e-01, time:   120\n",
      "step: 34200, loss: 5.3583e-01, time:   121\n",
      "step: 34300, loss: 5.3730e-01, time:   121\n",
      "step: 34400, loss: 5.3617e-01, time:   121\n",
      "step: 34500, loss: 5.3832e-01, time:   122\n",
      "step: 34600, loss: 5.3620e-01, time:   122\n",
      "step: 34700, loss: 5.3825e-01, time:   123\n",
      "step: 34800, loss: 5.3766e-01, time:   123\n",
      "step: 34900, loss: 5.4516e-01, time:   124\n",
      "step: 35000, loss: 5.3226e-01, time:   124\n",
      "step: 35100, loss: 5.3435e-01, time:   124\n",
      "step: 35200, loss: 5.3787e-01, time:   125\n",
      "step: 35300, loss: 5.3669e-01, time:   125\n",
      "step: 35400, loss: 5.3493e-01, time:   126\n",
      "step: 35500, loss: 5.3953e-01, time:   126\n",
      "step: 35600, loss: 5.3821e-01, time:   127\n",
      "step: 35700, loss: 5.3989e-01, time:   127\n",
      "step: 35800, loss: 5.3229e-01, time:   127\n",
      "step: 35900, loss: 5.3257e-01, time:   128\n",
      "step: 36000, loss: 5.3284e-01, time:   128\n",
      "step: 36100, loss: 5.3431e-01, time:   129\n",
      "step: 36200, loss: 5.3015e-01, time:   129\n",
      "step: 36300, loss: 5.3945e-01, time:   130\n",
      "step: 36400, loss: 5.3285e-01, time:   130\n",
      "step: 36500, loss: 5.3302e-01, time:   130\n",
      "step: 36600, loss: 5.2976e-01, time:   131\n",
      "step: 36700, loss: 5.3357e-01, time:   131\n",
      "step: 36800, loss: 5.3313e-01, time:   132\n",
      "step: 36900, loss: 5.2890e-01, time:   132\n",
      "step: 37000, loss: 5.2997e-01, time:   132\n",
      "step: 37100, loss: 5.3295e-01, time:   133\n",
      "step: 37200, loss: 5.3103e-01, time:   133\n",
      "step: 37300, loss: 5.3365e-01, time:   134\n",
      "step: 37400, loss: 5.3047e-01, time:   134\n",
      "step: 37500, loss: 5.3327e-01, time:   135\n",
      "step: 37600, loss: 5.3025e-01, time:   135\n",
      "step: 37700, loss: 5.2960e-01, time:   136\n",
      "step: 37800, loss: 5.3109e-01, time:   136\n",
      "step: 37900, loss: 5.3522e-01, time:   137\n",
      "step: 38000, loss: 5.2825e-01, time:   137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38100, loss: 5.8670e-01, time:   138\n",
      "step: 38200, loss: 5.3296e-01, time:   138\n",
      "step: 38300, loss: 5.3066e-01, time:   138\n",
      "step: 38400, loss: 5.2773e-01, time:   139\n",
      "step: 38500, loss: 5.2769e-01, time:   139\n",
      "step: 38600, loss: 5.2733e-01, time:   140\n",
      "step: 38700, loss: 5.2929e-01, time:   140\n",
      "step: 38800, loss: 5.2861e-01, time:   141\n",
      "step: 38900, loss: 5.2784e-01, time:   141\n",
      "step: 39000, loss: 5.3702e-01, time:   142\n",
      "step: 39100, loss: 5.3452e-01, time:   142\n",
      "step: 39200, loss: 5.2848e-01, time:   142\n",
      "step: 39300, loss: 5.3849e-01, time:   143\n",
      "step: 39400, loss: 5.2720e-01, time:   143\n",
      "step: 39500, loss: 5.4302e-01, time:   144\n",
      "step: 39600, loss: 5.2760e-01, time:   144\n",
      "step: 39700, loss: 5.2799e-01, time:   145\n",
      "step: 39800, loss: 5.2628e-01, time:   145\n",
      "step: 39900, loss: 5.2953e-01, time:   146\n",
      "step: 40000, loss: 5.3070e-01, time:   146\n",
      "step: 40100, loss: 5.3139e-01, time:   147\n",
      "step: 40200, loss: 5.2771e-01, time:   147\n",
      "step: 40300, loss: 5.2496e-01, time:   147\n",
      "step: 40400, loss: 5.2634e-01, time:   148\n",
      "step: 40500, loss: 5.2536e-01, time:   148\n",
      "step: 40600, loss: 5.2684e-01, time:   149\n",
      "step: 40700, loss: 5.2466e-01, time:   149\n",
      "step: 40800, loss: 5.2625e-01, time:   149\n",
      "step: 40900, loss: 5.2383e-01, time:   150\n",
      "step: 41000, loss: 5.2343e-01, time:   150\n",
      "step: 41100, loss: 5.2442e-01, time:   151\n",
      "step: 41200, loss: 5.5463e-01, time:   151\n",
      "step: 41300, loss: 5.3800e-01, time:   152\n",
      "step: 41400, loss: 5.3787e-01, time:   152\n",
      "step: 41500, loss: 5.4475e-01, time:   152\n",
      "step: 41600, loss: 5.3920e-01, time:   153\n",
      "step: 41700, loss: 5.2741e-01, time:   153\n",
      "step: 41800, loss: 5.2180e-01, time:   154\n",
      "step: 41900, loss: 5.2551e-01, time:   154\n",
      "step: 42000, loss: 5.2292e-01, time:   154\n",
      "step: 42100, loss: 5.2256e-01, time:   155\n",
      "step: 42200, loss: 5.3216e-01, time:   155\n",
      "step: 42300, loss: 5.2976e-01, time:   155\n",
      "step: 42400, loss: 5.4421e-01, time:   156\n",
      "step: 42500, loss: 5.5265e-01, time:   156\n",
      "step: 42600, loss: 5.4458e-01, time:   157\n",
      "step: 42700, loss: 5.4344e-01, time:   157\n",
      "step: 42800, loss: 5.4021e-01, time:   157\n",
      "step: 42900, loss: 5.4956e-01, time:   158\n",
      "step: 43000, loss: 5.3674e-01, time:   158\n",
      "step: 43100, loss: 5.4817e-01, time:   159\n",
      "step: 43200, loss: 5.3266e-01, time:   159\n",
      "step: 43300, loss: 5.3841e-01, time:   159\n",
      "step: 43400, loss: 5.3652e-01, time:   160\n",
      "step: 43500, loss: 5.3887e-01, time:   160\n",
      "step: 43600, loss: 5.3865e-01, time:   161\n",
      "step: 43700, loss: 5.3280e-01, time:   161\n",
      "step: 43800, loss: 5.3215e-01, time:   162\n",
      "step: 43900, loss: 5.3925e-01, time:   162\n",
      "step: 44000, loss: 5.3035e-01, time:   162\n",
      "step: 44100, loss: 5.2829e-01, time:   163\n",
      "step: 44200, loss: 5.3463e-01, time:   163\n",
      "step: 44300, loss: 5.4332e-01, time:   164\n",
      "step: 44400, loss: 5.3069e-01, time:   164\n",
      "step: 44500, loss: 5.4366e-01, time:   164\n",
      "step: 44600, loss: 5.2886e-01, time:   165\n",
      "step: 44700, loss: 5.3242e-01, time:   165\n",
      "step: 44800, loss: 5.4114e-01, time:   166\n",
      "step: 44900, loss: 5.3645e-01, time:   166\n",
      "step: 45000, loss: 5.3513e-01, time:   166\n",
      "step: 45100, loss: 5.3013e-01, time:   167\n",
      "step: 45200, loss: 5.3060e-01, time:   167\n",
      "step: 45300, loss: 5.3222e-01, time:   168\n",
      "step: 45400, loss: 5.2839e-01, time:   168\n",
      "step: 45500, loss: 5.4396e-01, time:   169\n",
      "step: 45600, loss: 5.2745e-01, time:   169\n",
      "step: 45700, loss: 5.2753e-01, time:   169\n",
      "step: 45800, loss: 5.3031e-01, time:   170\n",
      "step: 45900, loss: 5.2628e-01, time:   170\n",
      "step: 46000, loss: 5.2495e-01, time:   171\n",
      "step: 46100, loss: 5.2531e-01, time:   171\n",
      "step: 46200, loss: 5.2513e-01, time:   171\n",
      "step: 46300, loss: 5.2434e-01, time:   172\n",
      "step: 46400, loss: 5.3670e-01, time:   172\n",
      "step: 46500, loss: 5.2719e-01, time:   173\n",
      "step: 46600, loss: 5.2508e-01, time:   173\n",
      "step: 46700, loss: 5.3057e-01, time:   173\n",
      "step: 46800, loss: 5.2662e-01, time:   174\n",
      "step: 46900, loss: 5.2452e-01, time:   174\n",
      "step: 47000, loss: 5.2488e-01, time:   175\n",
      "step: 47100, loss: 5.2333e-01, time:   175\n",
      "step: 47200, loss: 5.2439e-01, time:   176\n",
      "step: 47300, loss: 5.2279e-01, time:   176\n",
      "step: 47400, loss: 5.2310e-01, time:   176\n",
      "step: 47500, loss: 5.2739e-01, time:   177\n",
      "step: 47600, loss: 5.4008e-01, time:   177\n",
      "step: 47700, loss: 5.2741e-01, time:   177\n",
      "step: 47800, loss: 5.2285e-01, time:   178\n",
      "step: 47900, loss: 5.4076e-01, time:   178\n",
      "step: 48000, loss: 5.2345e-01, time:   178\n",
      "step: 48100, loss: 5.2653e-01, time:   178\n",
      "step: 48200, loss: 5.2627e-01, time:   179\n",
      "step: 48300, loss: 5.3099e-01, time:   179\n",
      "step: 48400, loss: 5.3453e-01, time:   180\n",
      "step: 48500, loss: 5.2897e-01, time:   180\n",
      "step: 48600, loss: 5.3037e-01, time:   180\n",
      "step: 48700, loss: 5.3068e-01, time:   181\n",
      "step: 48800, loss: 5.3161e-01, time:   181\n",
      "step: 48900, loss: 5.2785e-01, time:   182\n",
      "step: 49000, loss: 5.3163e-01, time:   182\n",
      "step: 49100, loss: 5.4806e-01, time:   183\n",
      "step: 49200, loss: 5.2924e-01, time:   183\n",
      "step: 49300, loss: 5.2735e-01, time:   183\n",
      "step: 49400, loss: 5.2630e-01, time:   184\n",
      "step: 49500, loss: 5.2734e-01, time:   184\n",
      "step: 49600, loss: 5.3678e-01, time:   185\n",
      "step: 49700, loss: 5.3610e-01, time:   185\n",
      "step: 49800, loss: 5.2883e-01, time:   185\n",
      "step: 49900, loss: 5.2388e-01, time:   186\n",
      "step: 50000, loss: 5.2644e-01, time:   186\n",
      "step: 50100, loss: 5.2366e-01, time:   187\n",
      "step: 50200, loss: 5.2429e-01, time:   187\n",
      "step: 50300, loss: 5.2372e-01, time:   188\n",
      "step: 50400, loss: 5.2317e-01, time:   188\n",
      "step: 50500, loss: 5.2774e-01, time:   188\n",
      "step: 50600, loss: 5.2776e-01, time:   189\n",
      "step: 50700, loss: 5.2979e-01, time:   189\n",
      "step: 50800, loss: 5.2998e-01, time:   190\n",
      "step: 50900, loss: 5.4127e-01, time:   190\n",
      "step: 51000, loss: 5.2399e-01, time:   191\n",
      "step: 51100, loss: 5.2924e-01, time:   191\n",
      "step: 51200, loss: 5.2548e-01, time:   191\n",
      "step: 51300, loss: 5.2633e-01, time:   192\n",
      "step: 51400, loss: 5.2801e-01, time:   192\n",
      "step: 51500, loss: 5.5239e-01, time:   193\n",
      "step: 51600, loss: 5.2499e-01, time:   193\n",
      "step: 51700, loss: 5.2736e-01, time:   193\n",
      "step: 51800, loss: 5.2395e-01, time:   194\n",
      "step: 51900, loss: 5.2367e-01, time:   194\n",
      "step: 52000, loss: 5.2984e-01, time:   195\n",
      "step: 52100, loss: 5.2545e-01, time:   195\n",
      "step: 52200, loss: 5.2605e-01, time:   196\n",
      "step: 52300, loss: 5.3488e-01, time:   196\n",
      "step: 52400, loss: 5.2446e-01, time:   196\n",
      "step: 52500, loss: 5.2377e-01, time:   197\n",
      "step: 52600, loss: 5.3032e-01, time:   197\n",
      "step: 52700, loss: 5.2378e-01, time:   198\n",
      "step: 52800, loss: 5.2279e-01, time:   198\n",
      "step: 52900, loss: 5.2530e-01, time:   198\n",
      "step: 53000, loss: 5.2228e-01, time:   199\n",
      "step: 53100, loss: 5.2346e-01, time:   199\n",
      "step: 53200, loss: 5.2663e-01, time:   200\n",
      "step: 53300, loss: 5.2916e-01, time:   200\n",
      "step: 53400, loss: 5.2234e-01, time:   201\n",
      "step: 53500, loss: 5.2856e-01, time:   201\n",
      "step: 53600, loss: 5.2694e-01, time:   201\n",
      "step: 53700, loss: 5.2212e-01, time:   202\n",
      "step: 53800, loss: 5.2166e-01, time:   202\n",
      "step: 53900, loss: 5.2870e-01, time:   203\n",
      "step: 54000, loss: 5.2609e-01, time:   203\n",
      "step: 54100, loss: 5.2410e-01, time:   204\n",
      "step: 54200, loss: 5.2390e-01, time:   204\n",
      "step: 54300, loss: 5.2133e-01, time:   204\n",
      "step: 54400, loss: 5.2837e-01, time:   205\n",
      "step: 54500, loss: 5.2177e-01, time:   205\n",
      "step: 54600, loss: 5.3515e-01, time:   206\n",
      "step: 54700, loss: 5.2263e-01, time:   206\n",
      "step: 54800, loss: 5.2420e-01, time:   207\n",
      "step: 54900, loss: 5.2058e-01, time:   207\n",
      "step: 55000, loss: 5.2592e-01, time:   207\n",
      "step: 55100, loss: 5.5484e-01, time:   208\n",
      "step: 55200, loss: 5.3112e-01, time:   208\n",
      "step: 55300, loss: 5.1867e-01, time:   209\n",
      "step: 55400, loss: 5.2065e-01, time:   209\n",
      "step: 55500, loss: 5.1832e-01, time:   210\n",
      "step: 55600, loss: 5.2079e-01, time:   210\n",
      "step: 55700, loss: 5.1699e-01, time:   210\n",
      "step: 55800, loss: 5.4088e-01, time:   211\n",
      "step: 55900, loss: 5.1712e-01, time:   211\n",
      "step: 56000, loss: 5.1729e-01, time:   212\n",
      "step: 56100, loss: 5.1851e-01, time:   212\n",
      "step: 56200, loss: 5.1666e-01, time:   213\n",
      "step: 56300, loss: 5.1598e-01, time:   213\n",
      "step: 56400, loss: 5.2109e-01, time:   213\n",
      "step: 56500, loss: 5.1651e-01, time:   214\n",
      "step: 56600, loss: 5.1707e-01, time:   214\n",
      "step: 56700, loss: 5.2137e-01, time:   215\n",
      "step: 56800, loss: 5.1743e-01, time:   215\n",
      "step: 56900, loss: 5.2490e-01, time:   216\n",
      "step: 57000, loss: 5.1994e-01, time:   216\n",
      "step: 57100, loss: 5.2188e-01, time:   216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 57200, loss: 5.2058e-01, time:   217\n",
      "step: 57300, loss: 5.2099e-01, time:   217\n",
      "step: 57400, loss: 5.1723e-01, time:   218\n",
      "step: 57500, loss: 5.1975e-01, time:   218\n",
      "step: 57600, loss: 5.2048e-01, time:   218\n",
      "step: 57700, loss: 5.1832e-01, time:   219\n",
      "step: 57800, loss: 5.2290e-01, time:   219\n",
      "step: 57900, loss: 5.2031e-01, time:   220\n",
      "step: 58000, loss: 5.2054e-01, time:   220\n",
      "step: 58100, loss: 5.4664e-01, time:   221\n",
      "step: 58200, loss: 5.2270e-01, time:   221\n",
      "step: 58300, loss: 5.2619e-01, time:   221\n",
      "step: 58400, loss: 5.2297e-01, time:   222\n",
      "step: 58500, loss: 5.2139e-01, time:   222\n",
      "step: 58600, loss: 5.2110e-01, time:   223\n",
      "step: 58700, loss: 5.1995e-01, time:   223\n",
      "step: 58800, loss: 5.2046e-01, time:   224\n",
      "step: 58900, loss: 5.2099e-01, time:   224\n",
      "step: 59000, loss: 5.2482e-01, time:   224\n",
      "step: 59100, loss: 5.2675e-01, time:   225\n",
      "step: 59200, loss: 5.2300e-01, time:   225\n",
      "step: 59300, loss: 5.2007e-01, time:   226\n",
      "step: 59400, loss: 5.2370e-01, time:   226\n",
      "step: 59500, loss: 5.2193e-01, time:   227\n",
      "step: 59600, loss: 5.2074e-01, time:   227\n",
      "step: 59700, loss: 5.2817e-01, time:   227\n",
      "step: 59800, loss: 5.2114e-01, time:   228\n",
      "step: 59900, loss: 5.1794e-01, time:   228\n",
      "step: 60000, loss: 5.2298e-01, time:   229\n",
      "step: 60100, loss: 5.2528e-01, time:   229\n",
      "step: 60200, loss: 5.2374e-01, time:   230\n",
      "step: 60300, loss: 5.2278e-01, time:   230\n",
      "step: 60400, loss: 5.3503e-01, time:   230\n",
      "step: 60500, loss: 5.1992e-01, time:   231\n",
      "step: 60600, loss: 5.2473e-01, time:   231\n",
      "step: 60700, loss: 5.1857e-01, time:   232\n",
      "step: 60800, loss: 5.2657e-01, time:   232\n",
      "step: 60900, loss: 5.2339e-01, time:   233\n",
      "step: 61000, loss: 5.1461e-01, time:   233\n",
      "step: 61100, loss: 5.1292e-01, time:   233\n",
      "step: 61200, loss: 5.1316e-01, time:   234\n",
      "step: 61300, loss: 5.3068e-01, time:   234\n",
      "step: 61400, loss: 5.1975e-01, time:   235\n",
      "step: 61500, loss: 5.1852e-01, time:   235\n",
      "step: 61600, loss: 5.1159e-01, time:   236\n",
      "step: 61700, loss: 5.1162e-01, time:   236\n",
      "step: 61800, loss: 5.1684e-01, time:   236\n",
      "step: 61900, loss: 5.1743e-01, time:   237\n",
      "step: 62000, loss: 5.1426e-01, time:   237\n",
      "step: 62100, loss: 5.1720e-01, time:   238\n",
      "step: 62200, loss: 5.1141e-01, time:   238\n",
      "step: 62300, loss: 5.2056e-01, time:   238\n",
      "step: 62400, loss: 5.1550e-01, time:   239\n",
      "step: 62500, loss: 5.1184e-01, time:   239\n",
      "step: 62600, loss: 5.1450e-01, time:   240\n",
      "step: 62700, loss: 5.1115e-01, time:   240\n",
      "step: 62800, loss: 5.1564e-01, time:   241\n",
      "step: 62900, loss: 5.1332e-01, time:   241\n",
      "step: 63000, loss: 5.2428e-01, time:   241\n",
      "step: 63100, loss: 5.1276e-01, time:   242\n",
      "step: 63200, loss: 5.1614e-01, time:   242\n",
      "step: 63300, loss: 5.1355e-01, time:   243\n",
      "step: 63400, loss: 5.1219e-01, time:   243\n",
      "step: 63500, loss: 5.4240e-01, time:   244\n",
      "step: 63600, loss: 5.1546e-01, time:   244\n",
      "step: 63700, loss: 5.1306e-01, time:   245\n",
      "step: 63800, loss: 5.0915e-01, time:   245\n",
      "step: 63900, loss: 5.1018e-01, time:   245\n",
      "step: 64000, loss: 5.3604e-01, time:   246\n",
      "step: 64100, loss: 5.1679e-01, time:   246\n",
      "step: 64200, loss: 5.1092e-01, time:   247\n",
      "step: 64300, loss: 5.0998e-01, time:   247\n",
      "step: 64400, loss: 5.1036e-01, time:   247\n",
      "step: 64500, loss: 6.4520e-01, time:   248\n",
      "step: 64600, loss: 5.1872e-01, time:   248\n",
      "step: 64700, loss: 5.0954e-01, time:   249\n",
      "step: 64800, loss: 5.1333e-01, time:   249\n",
      "step: 64900, loss: 5.0913e-01, time:   250\n",
      "step: 65000, loss: 5.1021e-01, time:   250\n",
      "step: 65100, loss: 5.2069e-01, time:   250\n",
      "step: 65200, loss: 5.1004e-01, time:   251\n",
      "step: 65300, loss: 5.0941e-01, time:   251\n",
      "step: 65400, loss: 5.1240e-01, time:   252\n",
      "step: 65500, loss: 5.1119e-01, time:   252\n",
      "step: 65600, loss: 5.0961e-01, time:   252\n",
      "step: 65700, loss: 5.1407e-01, time:   253\n",
      "step: 65800, loss: 5.1471e-01, time:   253\n",
      "step: 65900, loss: 5.1144e-01, time:   254\n",
      "step: 66000, loss: 5.0916e-01, time:   254\n",
      "step: 66100, loss: 5.0855e-01, time:   254\n",
      "step: 66200, loss: 5.1190e-01, time:   255\n",
      "step: 66300, loss: 5.1029e-01, time:   255\n",
      "step: 66400, loss: 5.0948e-01, time:   256\n",
      "step: 66500, loss: 5.0926e-01, time:   256\n",
      "step: 66600, loss: 5.0962e-01, time:   257\n",
      "step: 66700, loss: 5.0867e-01, time:   257\n",
      "step: 66800, loss: 5.1133e-01, time:   257\n",
      "step: 66900, loss: 5.0843e-01, time:   257\n",
      "step: 67000, loss: 5.1836e-01, time:   258\n",
      "step: 67100, loss: 5.1118e-01, time:   258\n",
      "step: 67200, loss: 5.0825e-01, time:   258\n",
      "step: 67300, loss: 5.0918e-01, time:   258\n",
      "step: 67400, loss: 5.0960e-01, time:   259\n",
      "step: 67500, loss: 5.2901e-01, time:   259\n",
      "step: 67600, loss: 5.0834e-01, time:   260\n",
      "step: 67700, loss: 5.2767e-01, time:   260\n",
      "step: 67800, loss: 5.1390e-01, time:   260\n",
      "step: 67900, loss: 5.0730e-01, time:   261\n",
      "step: 68000, loss: 5.0905e-01, time:   261\n",
      "step: 68100, loss: 5.0980e-01, time:   262\n",
      "step: 68200, loss: 5.0706e-01, time:   262\n",
      "step: 68300, loss: 5.0748e-01, time:   263\n",
      "step: 68400, loss: 5.0718e-01, time:   263\n",
      "step: 68500, loss: 5.0750e-01, time:   263\n",
      "step: 68600, loss: 5.0750e-01, time:   264\n",
      "step: 68700, loss: 5.0870e-01, time:   264\n",
      "step: 68800, loss: 5.1979e-01, time:   265\n",
      "step: 68900, loss: 5.3545e-01, time:   265\n",
      "step: 69000, loss: 5.2041e-01, time:   266\n",
      "step: 69100, loss: 5.2142e-01, time:   266\n",
      "step: 69200, loss: 5.2188e-01, time:   267\n",
      "step: 69300, loss: 5.1645e-01, time:   267\n",
      "step: 69400, loss: 5.1497e-01, time:   268\n",
      "step: 69500, loss: 5.1867e-01, time:   268\n",
      "step: 69600, loss: 5.1860e-01, time:   269\n",
      "step: 69700, loss: 5.1567e-01, time:   269\n",
      "step: 69800, loss: 5.0592e-01, time:   269\n",
      "step: 69900, loss: 5.0898e-01, time:   270\n",
      "step: 70000, loss: 5.0619e-01, time:   270\n",
      "step: 70100, loss: 5.0971e-01, time:   271\n",
      "step: 70200, loss: 5.1217e-01, time:   271\n",
      "step: 70300, loss: 5.0618e-01, time:   272\n",
      "step: 70400, loss: 5.0692e-01, time:   272\n",
      "step: 70500, loss: 5.1254e-01, time:   273\n",
      "step: 70600, loss: 5.0679e-01, time:   273\n",
      "step: 70700, loss: 5.0587e-01, time:   273\n",
      "step: 70800, loss: 5.0721e-01, time:   274\n",
      "step: 70900, loss: 5.0595e-01, time:   274\n",
      "step: 71000, loss: 5.0747e-01, time:   275\n",
      "step: 71100, loss: 5.0650e-01, time:   275\n",
      "step: 71200, loss: 5.1719e-01, time:   276\n",
      "step: 71300, loss: 5.0465e-01, time:   276\n",
      "step: 71400, loss: 5.0609e-01, time:   276\n",
      "step: 71500, loss: 5.0506e-01, time:   277\n",
      "step: 71600, loss: 5.1542e-01, time:   277\n",
      "step: 71700, loss: 5.0818e-01, time:   278\n",
      "step: 71800, loss: 5.0778e-01, time:   278\n",
      "step: 71900, loss: 5.4602e-01, time:   279\n",
      "step: 72000, loss: 5.3789e-01, time:   279\n",
      "step: 72100, loss: 5.3345e-01, time:   279\n",
      "step: 72200, loss: 5.2695e-01, time:   280\n",
      "step: 72300, loss: 5.0820e-01, time:   280\n",
      "step: 72400, loss: 5.0615e-01, time:   281\n",
      "step: 72500, loss: 5.0659e-01, time:   281\n",
      "step: 72600, loss: 5.0619e-01, time:   282\n",
      "step: 72700, loss: 5.0636e-01, time:   282\n",
      "step: 72800, loss: 5.0599e-01, time:   282\n",
      "step: 72900, loss: 5.1544e-01, time:   283\n",
      "step: 73000, loss: 5.0730e-01, time:   283\n",
      "step: 73100, loss: 5.0833e-01, time:   284\n",
      "step: 73200, loss: 6.3310e-01, time:   284\n",
      "step: 73300, loss: 5.3996e-01, time:   285\n",
      "step: 73400, loss: 5.3555e-01, time:   285\n",
      "step: 73500, loss: 5.3169e-01, time:   286\n",
      "step: 73600, loss: 5.3301e-01, time:   286\n",
      "step: 73700, loss: 5.3086e-01, time:   286\n",
      "step: 73800, loss: 5.2856e-01, time:   287\n",
      "step: 73900, loss: 5.2764e-01, time:   287\n",
      "step: 74000, loss: 5.2756e-01, time:   288\n",
      "step: 74100, loss: 5.2768e-01, time:   288\n",
      "step: 74200, loss: 5.3188e-01, time:   288\n",
      "step: 74300, loss: 5.2656e-01, time:   289\n",
      "step: 74400, loss: 5.2925e-01, time:   289\n",
      "step: 74500, loss: 5.2584e-01, time:   290\n",
      "step: 74600, loss: 5.2629e-01, time:   290\n",
      "step: 74700, loss: 5.4012e-01, time:   291\n",
      "step: 74800, loss: 5.2538e-01, time:   291\n",
      "step: 74900, loss: 5.3190e-01, time:   292\n",
      "step: 75000, loss: 5.2888e-01, time:   292\n",
      "step: 75100, loss: 5.2699e-01, time:   292\n",
      "step: 75200, loss: 5.2891e-01, time:   293\n",
      "step: 75300, loss: 5.2515e-01, time:   293\n",
      "step: 75400, loss: 5.3255e-01, time:   294\n",
      "step: 75500, loss: 5.3071e-01, time:   294\n",
      "step: 75600, loss: 5.3038e-01, time:   295\n",
      "step: 75700, loss: 5.3149e-01, time:   295\n",
      "step: 75800, loss: 5.2973e-01, time:   295\n",
      "step: 75900, loss: 5.3313e-01, time:   296\n",
      "step: 76000, loss: 5.2916e-01, time:   296\n",
      "step: 76100, loss: 5.2894e-01, time:   297\n",
      "step: 76200, loss: 5.3353e-01, time:   297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 76300, loss: 5.3305e-01, time:   298\n",
      "step: 76400, loss: 5.3020e-01, time:   298\n",
      "step: 76500, loss: 5.3180e-01, time:   299\n",
      "step: 76600, loss: 5.3340e-01, time:   299\n",
      "step: 76700, loss: 5.3149e-01, time:   299\n",
      "step: 76800, loss: 5.2899e-01, time:   300\n",
      "step: 76900, loss: 5.3922e-01, time:   300\n",
      "step: 77000, loss: 5.3565e-01, time:   301\n",
      "step: 77100, loss: 5.3860e-01, time:   301\n",
      "step: 77200, loss: 5.2366e-01, time:   302\n",
      "step: 77300, loss: 5.2551e-01, time:   302\n",
      "step: 77400, loss: 5.2674e-01, time:   302\n",
      "step: 77500, loss: 5.3647e-01, time:   303\n",
      "step: 77600, loss: 5.3672e-01, time:   303\n",
      "step: 77700, loss: 5.3764e-01, time:   304\n",
      "step: 77800, loss: 5.4404e-01, time:   304\n",
      "step: 77900, loss: 5.2770e-01, time:   305\n",
      "step: 78000, loss: 5.2800e-01, time:   305\n",
      "step: 78100, loss: 5.2770e-01, time:   306\n",
      "step: 78200, loss: 5.2527e-01, time:   306\n",
      "step: 78300, loss: 5.2408e-01, time:   307\n",
      "step: 78400, loss: 5.2516e-01, time:   307\n",
      "step: 78500, loss: 5.2568e-01, time:   307\n",
      "step: 78600, loss: 5.1912e-01, time:   308\n",
      "step: 78700, loss: 5.1818e-01, time:   308\n",
      "step: 78800, loss: 5.0751e-01, time:   309\n",
      "step: 78900, loss: 5.1309e-01, time:   309\n",
      "step: 79000, loss: 5.0845e-01, time:   310\n",
      "step: 79100, loss: 5.0923e-01, time:   310\n",
      "step: 79200, loss: 5.0729e-01, time:   311\n",
      "step: 79300, loss: 5.4852e-01, time:   311\n",
      "step: 79400, loss: 5.2919e-01, time:   311\n",
      "step: 79500, loss: 5.2951e-01, time:   312\n",
      "step: 79600, loss: 5.3413e-01, time:   312\n",
      "step: 79700, loss: 5.3092e-01, time:   313\n",
      "step: 79800, loss: 5.2849e-01, time:   313\n",
      "step: 79900, loss: 5.2962e-01, time:   314\n",
      "step: 80000, loss: 5.1714e-01, time:   314\n",
      "step: 80100, loss: 5.1747e-01, time:   315\n",
      "step: 80200, loss: 5.0624e-01, time:   315\n",
      "step: 80300, loss: 5.0588e-01, time:   315\n",
      "step: 80400, loss: 5.0816e-01, time:   316\n",
      "step: 80500, loss: 5.0569e-01, time:   316\n",
      "step: 80600, loss: 5.3440e-01, time:   317\n",
      "step: 80700, loss: 5.2944e-01, time:   317\n",
      "step: 80800, loss: 5.3064e-01, time:   318\n",
      "step: 80900, loss: 5.2772e-01, time:   318\n",
      "step: 81000, loss: 5.3217e-01, time:   319\n",
      "step: 81100, loss: 5.3663e-01, time:   319\n",
      "step: 81200, loss: 5.2791e-01, time:   319\n",
      "step: 81300, loss: 5.2730e-01, time:   320\n",
      "step: 81400, loss: 5.3439e-01, time:   320\n",
      "step: 81500, loss: 5.2722e-01, time:   321\n",
      "step: 81600, loss: 5.2649e-01, time:   321\n",
      "step: 81700, loss: 5.3546e-01, time:   322\n",
      "step: 81800, loss: 5.2698e-01, time:   322\n",
      "step: 81900, loss: 5.2660e-01, time:   322\n",
      "step: 82000, loss: 5.2707e-01, time:   323\n",
      "step: 82100, loss: 5.3130e-01, time:   323\n",
      "step: 82200, loss: 5.2745e-01, time:   324\n",
      "step: 82300, loss: 5.2601e-01, time:   324\n",
      "step: 82400, loss: 5.2519e-01, time:   325\n",
      "step: 82500, loss: 5.2609e-01, time:   325\n",
      "step: 82600, loss: 5.3057e-01, time:   326\n",
      "step: 82700, loss: 5.2804e-01, time:   326\n",
      "step: 82800, loss: 5.2633e-01, time:   326\n",
      "step: 82900, loss: 5.2440e-01, time:   327\n",
      "step: 83000, loss: 5.3437e-01, time:   327\n",
      "step: 83100, loss: 5.2467e-01, time:   328\n",
      "step: 83200, loss: 5.2733e-01, time:   328\n",
      "step: 83300, loss: 5.3355e-01, time:   328\n",
      "step: 83400, loss: 5.2991e-01, time:   329\n",
      "step: 83500, loss: 5.2479e-01, time:   329\n",
      "step: 83600, loss: 5.2384e-01, time:   330\n",
      "step: 83700, loss: 5.2497e-01, time:   330\n",
      "step: 83800, loss: 5.2732e-01, time:   331\n",
      "step: 83900, loss: 5.2664e-01, time:   331\n",
      "step: 84000, loss: 5.2823e-01, time:   332\n",
      "step: 84100, loss: 5.2766e-01, time:   332\n",
      "step: 84200, loss: 5.2830e-01, time:   332\n",
      "step: 84300, loss: 5.2479e-01, time:   333\n",
      "step: 84400, loss: 5.3431e-01, time:   333\n",
      "step: 84500, loss: 5.2532e-01, time:   334\n",
      "step: 84600, loss: 5.2694e-01, time:   334\n",
      "step: 84700, loss: 5.2445e-01, time:   335\n",
      "step: 84800, loss: 5.2895e-01, time:   335\n",
      "step: 84900, loss: 5.2651e-01, time:   336\n",
      "step: 85000, loss: 5.3289e-01, time:   336\n",
      "step: 85100, loss: 5.2955e-01, time:   336\n",
      "step: 85200, loss: 5.4190e-01, time:   337\n",
      "step: 85300, loss: 5.5159e-01, time:   337\n",
      "step: 85400, loss: 5.3218e-01, time:   338\n",
      "step: 85500, loss: 5.2846e-01, time:   338\n",
      "step: 85600, loss: 5.3557e-01, time:   339\n",
      "step: 85700, loss: 5.3328e-01, time:   339\n",
      "step: 85800, loss: 5.3507e-01, time:   340\n",
      "step: 85900, loss: 5.3627e-01, time:   340\n",
      "step: 86000, loss: 5.3427e-01, time:   340\n",
      "step: 86100, loss: 5.2187e-01, time:   341\n",
      "step: 86200, loss: 5.2798e-01, time:   341\n",
      "step: 86300, loss: 5.3736e-01, time:   342\n",
      "step: 86400, loss: 5.1184e-01, time:   342\n",
      "step: 86500, loss: 5.1120e-01, time:   343\n",
      "step: 86600, loss: 5.0911e-01, time:   343\n",
      "step: 86700, loss: 5.1179e-01, time:   343\n",
      "step: 86800, loss: 5.0997e-01, time:   344\n",
      "step: 86900, loss: 5.2118e-01, time:   344\n",
      "step: 87000, loss: 5.0710e-01, time:   345\n",
      "step: 87100, loss: 5.0901e-01, time:   345\n",
      "step: 87200, loss: 5.0546e-01, time:   346\n",
      "step: 87300, loss: 5.0914e-01, time:   346\n",
      "step: 87400, loss: 5.0600e-01, time:   347\n",
      "step: 87500, loss: 5.0564e-01, time:   347\n",
      "step: 87600, loss: 5.0993e-01, time:   347\n",
      "step: 87700, loss: 5.0547e-01, time:   348\n",
      "step: 87800, loss: 5.0437e-01, time:   348\n",
      "step: 87900, loss: 5.1317e-01, time:   349\n",
      "step: 88000, loss: 5.0360e-01, time:   349\n",
      "step: 88100, loss: 5.0277e-01, time:   350\n",
      "step: 88200, loss: 5.0306e-01, time:   350\n",
      "step: 88300, loss: 5.0663e-01, time:   351\n",
      "step: 88400, loss: 5.0997e-01, time:   351\n",
      "step: 88500, loss: 5.0897e-01, time:   352\n",
      "step: 88600, loss: 5.3262e-01, time:   352\n",
      "step: 88700, loss: 5.0376e-01, time:   352\n",
      "step: 88800, loss: 5.0171e-01, time:   353\n",
      "step: 88900, loss: 5.0092e-01, time:   353\n",
      "step: 89000, loss: 5.1036e-01, time:   354\n",
      "step: 89100, loss: 5.0158e-01, time:   354\n",
      "step: 89200, loss: 5.0262e-01, time:   355\n",
      "step: 89300, loss: 5.0374e-01, time:   355\n",
      "step: 89400, loss: 5.0127e-01, time:   356\n",
      "step: 89500, loss: 5.0784e-01, time:   356\n",
      "step: 89600, loss: 5.0177e-01, time:   356\n",
      "step: 89700, loss: 5.0128e-01, time:   357\n",
      "step: 89800, loss: 5.0264e-01, time:   357\n",
      "step: 89900, loss: 5.0134e-01, time:   358\n",
      "step: 90000, loss: 4.9897e-01, time:   358\n",
      "step: 90100, loss: 4.9963e-01, time:   359\n",
      "step: 90200, loss: 5.0093e-01, time:   359\n",
      "step: 90300, loss: 4.9873e-01, time:   359\n",
      "step: 90400, loss: 4.9889e-01, time:   360\n",
      "step: 90500, loss: 4.9842e-01, time:   360\n",
      "step: 90600, loss: 5.0210e-01, time:   361\n",
      "step: 90700, loss: 5.0367e-01, time:   361\n",
      "step: 90800, loss: 4.9802e-01, time:   362\n",
      "step: 90900, loss: 4.9838e-01, time:   362\n",
      "step: 91000, loss: 4.9811e-01, time:   363\n",
      "step: 91100, loss: 4.9899e-01, time:   363\n",
      "step: 91200, loss: 4.9855e-01, time:   363\n",
      "step: 91300, loss: 4.9804e-01, time:   364\n",
      "step: 91400, loss: 4.9804e-01, time:   364\n",
      "step: 91500, loss: 4.9912e-01, time:   365\n",
      "step: 91600, loss: 4.9967e-01, time:   365\n",
      "step: 91700, loss: 4.9840e-01, time:   366\n",
      "step: 91800, loss: 4.9787e-01, time:   366\n",
      "step: 91900, loss: 4.9917e-01, time:   367\n",
      "step: 92000, loss: 5.0317e-01, time:   367\n",
      "step: 92100, loss: 4.9806e-01, time:   368\n",
      "step: 92200, loss: 4.9686e-01, time:   368\n",
      "step: 92300, loss: 5.0074e-01, time:   369\n",
      "step: 92400, loss: 4.9827e-01, time:   369\n",
      "step: 92500, loss: 4.9919e-01, time:   369\n",
      "step: 92600, loss: 5.0600e-01, time:   370\n",
      "step: 92700, loss: 4.9982e-01, time:   370\n",
      "step: 92800, loss: 5.0398e-01, time:   371\n",
      "step: 92900, loss: 5.2651e-01, time:   371\n",
      "step: 93000, loss: 5.0921e-01, time:   372\n",
      "step: 93100, loss: 5.0105e-01, time:   372\n",
      "step: 93200, loss: 4.9963e-01, time:   373\n",
      "step: 93300, loss: 5.4890e-01, time:   373\n",
      "step: 93400, loss: 5.3409e-01, time:   373\n",
      "step: 93500, loss: 5.3123e-01, time:   374\n",
      "step: 93600, loss: 5.3898e-01, time:   374\n",
      "step: 93700, loss: 5.5098e-01, time:   375\n",
      "step: 93800, loss: 5.4414e-01, time:   375\n",
      "step: 93900, loss: 5.0535e-01, time:   376\n",
      "step: 94000, loss: 5.0284e-01, time:   376\n",
      "step: 94100, loss: 5.0090e-01, time:   377\n",
      "step: 94200, loss: 5.0108e-01, time:   377\n",
      "step: 94300, loss: 5.0039e-01, time:   377\n",
      "step: 94400, loss: 5.0309e-01, time:   378\n",
      "step: 94500, loss: 4.9978e-01, time:   378\n",
      "step: 94600, loss: 5.0059e-01, time:   379\n",
      "step: 94700, loss: 4.9906e-01, time:   379\n",
      "step: 94800, loss: 4.9990e-01, time:   380\n",
      "step: 94900, loss: 4.9924e-01, time:   380\n",
      "step: 95000, loss: 4.9883e-01, time:   381\n",
      "step: 95100, loss: 5.0079e-01, time:   381\n",
      "step: 95200, loss: 5.0243e-01, time:   382\n",
      "step: 95300, loss: 4.9987e-01, time:   382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 95400, loss: 4.9919e-01, time:   382\n",
      "step: 95500, loss: 4.9868e-01, time:   383\n",
      "step: 95600, loss: 5.0377e-01, time:   383\n",
      "step: 95700, loss: 5.0407e-01, time:   384\n",
      "step: 95800, loss: 4.9994e-01, time:   384\n",
      "step: 95900, loss: 4.9809e-01, time:   385\n",
      "step: 96000, loss: 4.9884e-01, time:   385\n",
      "step: 96100, loss: 5.0754e-01, time:   386\n",
      "step: 96200, loss: 4.9967e-01, time:   386\n",
      "step: 96300, loss: 4.9929e-01, time:   386\n",
      "step: 96400, loss: 5.0031e-01, time:   387\n",
      "step: 96500, loss: 4.9947e-01, time:   387\n",
      "step: 96600, loss: 4.9814e-01, time:   388\n",
      "step: 96700, loss: 4.9955e-01, time:   388\n",
      "step: 96800, loss: 4.9761e-01, time:   389\n",
      "step: 96900, loss: 5.0022e-01, time:   389\n",
      "step: 97000, loss: 4.9985e-01, time:   390\n",
      "step: 97100, loss: 4.9710e-01, time:   390\n",
      "step: 97200, loss: 4.9680e-01, time:   390\n",
      "step: 97300, loss: 5.0611e-01, time:   391\n",
      "step: 97400, loss: 4.9842e-01, time:   391\n",
      "step: 97500, loss: 5.0196e-01, time:   392\n",
      "step: 97600, loss: 5.0254e-01, time:   392\n",
      "step: 97700, loss: 4.9889e-01, time:   393\n",
      "step: 97800, loss: 4.9834e-01, time:   393\n",
      "step: 97900, loss: 4.9687e-01, time:   394\n",
      "step: 98000, loss: 4.9769e-01, time:   394\n",
      "step: 98100, loss: 5.0174e-01, time:   394\n",
      "step: 98200, loss: 4.9691e-01, time:   395\n",
      "step: 98300, loss: 4.9821e-01, time:   395\n",
      "step: 98400, loss: 4.9709e-01, time:   396\n",
      "step: 98500, loss: 4.9831e-01, time:   396\n",
      "step: 98600, loss: 4.9809e-01, time:   397\n",
      "step: 98700, loss: 5.0239e-01, time:   397\n",
      "step: 98800, loss: 4.9975e-01, time:   398\n",
      "step: 98900, loss: 5.0277e-01, time:   398\n",
      "step: 99000, loss: 5.0565e-01, time:   398\n",
      "step: 99100, loss: 5.0006e-01, time:   399\n",
      "step: 99200, loss: 4.9695e-01, time:   399\n",
      "step: 99300, loss: 4.9593e-01, time:   400\n",
      "step: 99400, loss: 4.9740e-01, time:   400\n",
      "step: 99500, loss: 4.9929e-01, time:   401\n",
      "step: 99600, loss: 5.0413e-01, time:   401\n",
      "step: 99700, loss: 4.9754e-01, time:   401\n",
      "step: 99800, loss: 4.9610e-01, time:   402\n",
      "step: 99900, loss: 4.9661e-01, time:   402\n",
      "step: 100000, loss: 4.9717e-01, time:   403\n",
      "step: 100100, loss: 5.0448e-01, time:   403\n",
      "step: 100200, loss: 4.9615e-01, time:   404\n",
      "step: 100300, loss: 4.9832e-01, time:   404\n",
      "step: 100400, loss: 4.9567e-01, time:   405\n",
      "step: 100500, loss: 4.9599e-01, time:   405\n",
      "step: 100600, loss: 4.9806e-01, time:   405\n",
      "step: 100700, loss: 5.0328e-01, time:   406\n",
      "step: 100800, loss: 4.9636e-01, time:   406\n",
      "step: 100900, loss: 4.9677e-01, time:   407\n",
      "step: 101000, loss: 5.1328e-01, time:   407\n",
      "step: 101100, loss: 5.2806e-01, time:   408\n",
      "step: 101200, loss: 5.3024e-01, time:   408\n",
      "step: 101300, loss: 5.2666e-01, time:   409\n",
      "step: 101400, loss: 5.2881e-01, time:   409\n",
      "step: 101500, loss: 5.7492e-01, time:   409\n",
      "step: 101600, loss: 5.1712e-01, time:   410\n",
      "step: 101700, loss: 5.1655e-01, time:   410\n",
      "step: 101800, loss: 5.2072e-01, time:   411\n",
      "step: 101900, loss: 5.2516e-01, time:   411\n",
      "step: 102000, loss: 5.2502e-01, time:   412\n",
      "step: 102100, loss: 5.2152e-01, time:   412\n",
      "step: 102200, loss: 5.2216e-01, time:   412\n",
      "step: 102300, loss: 5.2115e-01, time:   413\n",
      "step: 102400, loss: 5.2101e-01, time:   413\n",
      "step: 102500, loss: 5.2216e-01, time:   414\n",
      "step: 102600, loss: 5.2319e-01, time:   414\n",
      "step: 102700, loss: 5.2043e-01, time:   415\n",
      "step: 102800, loss: 5.2215e-01, time:   415\n",
      "step: 102900, loss: 5.1808e-01, time:   416\n",
      "step: 103000, loss: 5.1797e-01, time:   416\n",
      "step: 103100, loss: 5.1999e-01, time:   416\n",
      "step: 103200, loss: 5.0270e-01, time:   417\n",
      "step: 103300, loss: 5.0128e-01, time:   417\n",
      "step: 103400, loss: 5.0068e-01, time:   418\n",
      "step: 103500, loss: 5.0451e-01, time:   418\n",
      "step: 103600, loss: 5.0203e-01, time:   419\n",
      "step: 103700, loss: 5.1436e-01, time:   419\n",
      "step: 103800, loss: 4.9970e-01, time:   420\n",
      "step: 103900, loss: 5.0414e-01, time:   420\n",
      "step: 104000, loss: 5.0293e-01, time:   420\n",
      "step: 104100, loss: 4.9958e-01, time:   421\n",
      "step: 104200, loss: 5.0562e-01, time:   421\n",
      "step: 104300, loss: 5.0222e-01, time:   422\n",
      "step: 104400, loss: 4.9581e-01, time:   422\n",
      "step: 104500, loss: 5.0199e-01, time:   423\n",
      "step: 104600, loss: 5.0261e-01, time:   423\n",
      "step: 104700, loss: 4.9733e-01, time:   424\n",
      "step: 104800, loss: 4.9550e-01, time:   424\n",
      "step: 104900, loss: 4.9504e-01, time:   425\n",
      "step: 105000, loss: 4.9266e-01, time:   425\n",
      "step: 105100, loss: 4.9380e-01, time:   425\n",
      "step: 105200, loss: 4.9237e-01, time:   426\n",
      "step: 105300, loss: 4.9231e-01, time:   426\n",
      "step: 105400, loss: 4.9255e-01, time:   427\n",
      "step: 105500, loss: 4.9463e-01, time:   427\n",
      "step: 105600, loss: 4.9379e-01, time:   428\n",
      "step: 105700, loss: 4.9552e-01, time:   428\n",
      "step: 105800, loss: 5.0238e-01, time:   429\n",
      "step: 105900, loss: 4.9664e-01, time:   429\n",
      "step: 106000, loss: 4.9613e-01, time:   429\n",
      "step: 106100, loss: 4.9360e-01, time:   430\n",
      "step: 106200, loss: 4.9735e-01, time:   430\n",
      "step: 106300, loss: 4.9398e-01, time:   431\n",
      "step: 106400, loss: 4.9285e-01, time:   431\n",
      "step: 106500, loss: 4.9522e-01, time:   432\n",
      "step: 106600, loss: 4.9328e-01, time:   432\n",
      "step: 106700, loss: 5.0226e-01, time:   433\n",
      "step: 106800, loss: 5.0047e-01, time:   433\n",
      "step: 106900, loss: 4.9875e-01, time:   433\n",
      "step: 107000, loss: 4.9965e-01, time:   434\n",
      "step: 107100, loss: 4.9658e-01, time:   434\n",
      "step: 107200, loss: 4.9579e-01, time:   435\n",
      "step: 107300, loss: 4.9692e-01, time:   435\n",
      "step: 107400, loss: 5.1682e-01, time:   436\n",
      "step: 107500, loss: 4.9933e-01, time:   436\n",
      "step: 107600, loss: 4.9655e-01, time:   437\n",
      "step: 107700, loss: 4.9834e-01, time:   437\n",
      "step: 107800, loss: 5.2304e-01, time:   438\n",
      "step: 107900, loss: 4.9558e-01, time:   438\n",
      "step: 108000, loss: 4.9531e-01, time:   439\n",
      "step: 108100, loss: 4.9900e-01, time:   439\n",
      "step: 108200, loss: 5.1634e-01, time:   439\n",
      "step: 108300, loss: 4.9671e-01, time:   440\n",
      "step: 108400, loss: 5.0025e-01, time:   440\n",
      "step: 108500, loss: 5.1226e-01, time:   441\n",
      "step: 108600, loss: 5.0453e-01, time:   441\n",
      "step: 108700, loss: 4.9751e-01, time:   442\n",
      "step: 108800, loss: 5.0065e-01, time:   442\n",
      "step: 108900, loss: 4.9552e-01, time:   443\n",
      "step: 109000, loss: 4.9748e-01, time:   443\n",
      "step: 109100, loss: 4.9672e-01, time:   444\n",
      "step: 109200, loss: 4.9984e-01, time:   444\n",
      "step: 109300, loss: 5.0828e-01, time:   444\n",
      "step: 109400, loss: 4.9258e-01, time:   445\n",
      "step: 109500, loss: 4.9528e-01, time:   445\n",
      "step: 109600, loss: 4.9114e-01, time:   446\n",
      "step: 109700, loss: 4.9157e-01, time:   446\n",
      "step: 109800, loss: 4.8944e-01, time:   447\n",
      "step: 109900, loss: 4.9696e-01, time:   447\n",
      "step: 110000, loss: 4.9108e-01, time:   448\n",
      "step: 110100, loss: 4.9143e-01, time:   448\n",
      "step: 110200, loss: 4.9123e-01, time:   449\n",
      "step: 110300, loss: 4.9318e-01, time:   449\n",
      "step: 110400, loss: 5.0007e-01, time:   449\n",
      "step: 110500, loss: 4.9140e-01, time:   450\n",
      "step: 110600, loss: 4.9062e-01, time:   450\n",
      "step: 110700, loss: 4.9215e-01, time:   451\n",
      "step: 110800, loss: 4.9006e-01, time:   451\n",
      "step: 110900, loss: 4.9384e-01, time:   452\n",
      "step: 111000, loss: 4.8960e-01, time:   452\n",
      "step: 111100, loss: 4.9049e-01, time:   453\n",
      "step: 111200, loss: 4.8858e-01, time:   453\n",
      "step: 111300, loss: 4.9276e-01, time:   453\n",
      "step: 111400, loss: 4.9698e-01, time:   454\n",
      "step: 111500, loss: 4.9268e-01, time:   454\n",
      "step: 111600, loss: 4.9446e-01, time:   455\n",
      "step: 111700, loss: 4.8903e-01, time:   455\n",
      "step: 111800, loss: 4.9075e-01, time:   455\n",
      "step: 111900, loss: 4.9889e-01, time:   456\n",
      "step: 112000, loss: 4.9493e-01, time:   456\n",
      "step: 112100, loss: 4.8940e-01, time:   457\n",
      "step: 112200, loss: 4.8972e-01, time:   457\n",
      "step: 112300, loss: 4.9016e-01, time:   458\n",
      "step: 112400, loss: 4.8994e-01, time:   458\n",
      "step: 112500, loss: 4.9674e-01, time:   459\n",
      "step: 112600, loss: 4.9090e-01, time:   459\n",
      "step: 112700, loss: 4.9017e-01, time:   460\n",
      "step: 112800, loss: 4.9254e-01, time:   460\n",
      "step: 112900, loss: 5.0319e-01, time:   460\n",
      "step: 113000, loss: 5.3173e-01, time:   461\n",
      "step: 113100, loss: 5.2790e-01, time:   461\n",
      "step: 113200, loss: 4.9630e-01, time:   462\n",
      "step: 113300, loss: 4.9506e-01, time:   462\n",
      "step: 113400, loss: 4.9457e-01, time:   463\n",
      "step: 113500, loss: 5.0961e-01, time:   463\n",
      "step: 113600, loss: 4.9964e-01, time:   464\n",
      "step: 113700, loss: 4.9330e-01, time:   464\n",
      "step: 113800, loss: 4.9549e-01, time:   465\n",
      "step: 113900, loss: 4.9489e-01, time:   465\n",
      "step: 114000, loss: 4.9240e-01, time:   465\n",
      "step: 114100, loss: 4.9175e-01, time:   466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 114200, loss: 4.9655e-01, time:   466\n",
      "step: 114300, loss: 4.9289e-01, time:   467\n",
      "step: 114400, loss: 4.9686e-01, time:   467\n",
      "step: 114500, loss: 4.9335e-01, time:   468\n",
      "step: 114600, loss: 4.9280e-01, time:   468\n",
      "step: 114700, loss: 4.9312e-01, time:   469\n",
      "step: 114800, loss: 4.9497e-01, time:   469\n",
      "step: 114900, loss: 4.9318e-01, time:   469\n",
      "step: 115000, loss: 4.9033e-01, time:   470\n",
      "step: 115100, loss: 4.9163e-01, time:   470\n",
      "step: 115200, loss: 4.8932e-01, time:   471\n",
      "step: 115300, loss: 4.9525e-01, time:   471\n",
      "step: 115400, loss: 4.8954e-01, time:   472\n",
      "step: 115500, loss: 4.9255e-01, time:   472\n",
      "step: 115600, loss: 4.9228e-01, time:   473\n",
      "step: 115700, loss: 4.9188e-01, time:   473\n",
      "step: 115800, loss: 4.9215e-01, time:   473\n",
      "step: 115900, loss: 4.9299e-01, time:   474\n",
      "step: 116000, loss: 4.8922e-01, time:   474\n",
      "step: 116100, loss: 4.9141e-01, time:   475\n",
      "step: 116200, loss: 4.8900e-01, time:   475\n",
      "step: 116300, loss: 4.9183e-01, time:   476\n",
      "step: 116400, loss: 4.9511e-01, time:   476\n",
      "step: 116500, loss: 4.9160e-01, time:   477\n",
      "step: 116600, loss: 4.8962e-01, time:   477\n",
      "step: 116700, loss: 4.8951e-01, time:   478\n",
      "step: 116800, loss: 4.8887e-01, time:   478\n",
      "step: 116900, loss: 4.8887e-01, time:   478\n",
      "step: 117000, loss: 4.8960e-01, time:   479\n",
      "step: 117100, loss: 4.8974e-01, time:   479\n",
      "step: 117200, loss: 4.8930e-01, time:   480\n",
      "step: 117300, loss: 4.9002e-01, time:   480\n",
      "step: 117400, loss: 4.9038e-01, time:   481\n",
      "step: 117500, loss: 4.9498e-01, time:   481\n",
      "step: 117600, loss: 4.9349e-01, time:   482\n",
      "step: 117700, loss: 4.9126e-01, time:   482\n",
      "step: 117800, loss: 4.8893e-01, time:   482\n",
      "step: 117900, loss: 4.9053e-01, time:   483\n",
      "step: 118000, loss: 4.8716e-01, time:   483\n",
      "step: 118100, loss: 4.8909e-01, time:   484\n",
      "step: 118200, loss: 4.8776e-01, time:   484\n",
      "step: 118300, loss: 4.9105e-01, time:   485\n",
      "step: 118400, loss: 4.9015e-01, time:   485\n",
      "step: 118500, loss: 4.8874e-01, time:   486\n",
      "step: 118600, loss: 4.8845e-01, time:   486\n",
      "step: 118700, loss: 4.8806e-01, time:   487\n",
      "step: 118800, loss: 4.8935e-01, time:   487\n",
      "step: 118900, loss: 4.9717e-01, time:   488\n",
      "step: 119000, loss: 5.1588e-01, time:   488\n",
      "step: 119100, loss: 5.1765e-01, time:   488\n",
      "step: 119200, loss: 5.1333e-01, time:   489\n",
      "step: 119300, loss: 5.1530e-01, time:   489\n",
      "step: 119400, loss: 5.1246e-01, time:   490\n",
      "step: 119500, loss: 5.1229e-01, time:   490\n",
      "step: 119600, loss: 5.0892e-01, time:   491\n",
      "step: 119700, loss: 5.1052e-01, time:   491\n",
      "step: 119800, loss: 5.1620e-01, time:   492\n",
      "step: 119900, loss: 5.1773e-01, time:   492\n",
      "step: 120000, loss: 5.1022e-01, time:   492\n",
      "step: 120100, loss: 5.0735e-01, time:   493\n",
      "step: 120200, loss: 5.0657e-01, time:   493\n",
      "step: 120300, loss: 5.0606e-01, time:   494\n",
      "step: 120400, loss: 5.0440e-01, time:   494\n",
      "step: 120500, loss: 5.0410e-01, time:   495\n",
      "step: 120600, loss: 5.0381e-01, time:   495\n",
      "step: 120700, loss: 5.0282e-01, time:   496\n",
      "step: 120800, loss: 5.0444e-01, time:   496\n",
      "step: 120900, loss: 5.0097e-01, time:   496\n",
      "step: 121000, loss: 5.0396e-01, time:   497\n",
      "step: 121100, loss: 5.0837e-01, time:   497\n",
      "step: 121200, loss: 5.0144e-01, time:   498\n",
      "step: 121300, loss: 5.0004e-01, time:   498\n",
      "step: 121400, loss: 5.0581e-01, time:   499\n",
      "step: 121500, loss: 4.9718e-01, time:   499\n",
      "step: 121600, loss: 4.9875e-01, time:   499\n",
      "step: 121700, loss: 4.9803e-01, time:   500\n",
      "step: 121800, loss: 4.9770e-01, time:   500\n",
      "step: 121900, loss: 4.9960e-01, time:   501\n",
      "step: 122000, loss: 5.0270e-01, time:   501\n",
      "step: 122100, loss: 4.9822e-01, time:   502\n",
      "step: 122200, loss: 4.9803e-01, time:   502\n",
      "step: 122300, loss: 4.9679e-01, time:   503\n",
      "step: 122400, loss: 4.9632e-01, time:   503\n",
      "step: 122500, loss: 4.9923e-01, time:   503\n",
      "step: 122600, loss: 4.9832e-01, time:   504\n",
      "step: 122700, loss: 5.0005e-01, time:   504\n",
      "step: 122800, loss: 4.9888e-01, time:   505\n",
      "step: 122900, loss: 4.9643e-01, time:   505\n",
      "step: 123000, loss: 5.0047e-01, time:   506\n",
      "step: 123100, loss: 4.9763e-01, time:   506\n",
      "step: 123200, loss: 4.9886e-01, time:   507\n",
      "step: 123300, loss: 4.9508e-01, time:   507\n",
      "step: 123400, loss: 4.9416e-01, time:   507\n",
      "step: 123500, loss: 4.9627e-01, time:   508\n",
      "step: 123600, loss: 4.9842e-01, time:   508\n",
      "step: 123700, loss: 4.9760e-01, time:   509\n",
      "step: 123800, loss: 4.9550e-01, time:   509\n",
      "step: 123900, loss: 4.9775e-01, time:   510\n",
      "step: 124000, loss: 4.9473e-01, time:   510\n",
      "step: 124100, loss: 5.2680e-01, time:   511\n",
      "step: 124200, loss: 5.1605e-01, time:   511\n",
      "step: 124300, loss: 5.1613e-01, time:   511\n",
      "step: 124400, loss: 5.2179e-01, time:   512\n",
      "step: 124500, loss: 5.2369e-01, time:   512\n",
      "step: 124600, loss: 5.2613e-01, time:   513\n",
      "step: 124700, loss: 5.0159e-01, time:   513\n",
      "step: 124800, loss: 5.0046e-01, time:   514\n",
      "step: 124900, loss: 4.9637e-01, time:   514\n",
      "step: 125000, loss: 4.9595e-01, time:   515\n",
      "step: 125100, loss: 5.0676e-01, time:   515\n",
      "step: 125200, loss: 4.9798e-01, time:   516\n",
      "step: 125300, loss: 4.9544e-01, time:   516\n",
      "step: 125400, loss: 4.9311e-01, time:   516\n",
      "step: 125500, loss: 4.9224e-01, time:   517\n",
      "step: 125600, loss: 5.9912e-01, time:   517\n",
      "step: 125700, loss: 5.0551e-01, time:   518\n",
      "step: 125800, loss: 4.9449e-01, time:   518\n",
      "step: 125900, loss: 5.0118e-01, time:   519\n",
      "step: 126000, loss: 4.9364e-01, time:   519\n",
      "step: 126100, loss: 4.9440e-01, time:   520\n",
      "step: 126200, loss: 4.9396e-01, time:   520\n",
      "step: 126300, loss: 5.0902e-01, time:   520\n",
      "step: 126400, loss: 4.9620e-01, time:   521\n",
      "step: 126500, loss: 4.9673e-01, time:   521\n",
      "step: 126600, loss: 4.9528e-01, time:   522\n",
      "step: 126700, loss: 4.9984e-01, time:   522\n",
      "step: 126800, loss: 4.9662e-01, time:   523\n",
      "step: 126900, loss: 4.9479e-01, time:   523\n",
      "step: 127000, loss: 4.9607e-01, time:   524\n",
      "step: 127100, loss: 5.0430e-01, time:   524\n",
      "step: 127200, loss: 4.9552e-01, time:   525\n",
      "step: 127300, loss: 5.2122e-01, time:   525\n",
      "step: 127400, loss: 5.2106e-01, time:   525\n",
      "step: 127500, loss: 5.0943e-01, time:   526\n",
      "step: 127600, loss: 4.9653e-01, time:   526\n",
      "step: 127700, loss: 4.9152e-01, time:   527\n",
      "step: 127800, loss: 4.9002e-01, time:   527\n",
      "step: 127900, loss: 4.9096e-01, time:   528\n",
      "step: 128000, loss: 4.9492e-01, time:   528\n",
      "step: 128100, loss: 4.8905e-01, time:   529\n",
      "step: 128200, loss: 4.9088e-01, time:   529\n",
      "step: 128300, loss: 4.9016e-01, time:   529\n",
      "step: 128400, loss: 4.9732e-01, time:   530\n",
      "step: 128500, loss: 4.9077e-01, time:   530\n",
      "step: 128600, loss: 4.9411e-01, time:   531\n",
      "step: 128700, loss: 4.8982e-01, time:   531\n",
      "step: 128800, loss: 5.0052e-01, time:   532\n",
      "step: 128900, loss: 4.9336e-01, time:   532\n",
      "step: 129000, loss: 5.0049e-01, time:   533\n",
      "step: 129100, loss: 4.8907e-01, time:   533\n",
      "step: 129200, loss: 4.9052e-01, time:   534\n",
      "step: 129300, loss: 4.8974e-01, time:   534\n",
      "step: 129400, loss: 4.8958e-01, time:   534\n",
      "step: 129500, loss: 4.9206e-01, time:   535\n",
      "step: 129600, loss: 4.8867e-01, time:   535\n",
      "step: 129700, loss: 4.8988e-01, time:   536\n",
      "step: 129800, loss: 4.8823e-01, time:   536\n",
      "step: 129900, loss: 4.8975e-01, time:   537\n",
      "step: 130000, loss: 4.8800e-01, time:   537\n",
      "step: 130100, loss: 4.8817e-01, time:   538\n",
      "step: 130200, loss: 4.8931e-01, time:   538\n",
      "step: 130300, loss: 4.8845e-01, time:   539\n",
      "step: 130400, loss: 4.8904e-01, time:   539\n",
      "step: 130500, loss: 4.9063e-01, time:   539\n",
      "step: 130600, loss: 4.9327e-01, time:   540\n",
      "step: 130700, loss: 4.9107e-01, time:   540\n",
      "step: 130800, loss: 4.8992e-01, time:   541\n",
      "step: 130900, loss: 4.9189e-01, time:   541\n",
      "step: 131000, loss: 4.9057e-01, time:   542\n",
      "step: 131100, loss: 4.8801e-01, time:   542\n",
      "step: 131200, loss: 4.9088e-01, time:   543\n",
      "step: 131300, loss: 4.8980e-01, time:   543\n",
      "step: 131400, loss: 4.8947e-01, time:   544\n",
      "step: 131500, loss: 4.8844e-01, time:   544\n",
      "step: 131600, loss: 4.8777e-01, time:   544\n",
      "step: 131700, loss: 4.8855e-01, time:   545\n",
      "step: 131800, loss: 4.9579e-01, time:   545\n",
      "step: 131900, loss: 5.0683e-01, time:   546\n",
      "step: 132000, loss: 4.9214e-01, time:   546\n",
      "step: 132100, loss: 4.8696e-01, time:   547\n",
      "step: 132200, loss: 4.8663e-01, time:   547\n",
      "step: 132300, loss: 4.9778e-01, time:   548\n",
      "step: 132400, loss: 4.9273e-01, time:   548\n",
      "step: 132500, loss: 4.8893e-01, time:   549\n",
      "step: 132600, loss: 4.9364e-01, time:   549\n",
      "step: 132700, loss: 4.9019e-01, time:   549\n",
      "step: 132800, loss: 4.9328e-01, time:   550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 132900, loss: 4.9296e-01, time:   550\n",
      "step: 133000, loss: 4.8951e-01, time:   551\n",
      "step: 133100, loss: 4.9838e-01, time:   551\n",
      "step: 133200, loss: 4.9260e-01, time:   552\n",
      "step: 133300, loss: 4.8799e-01, time:   552\n",
      "step: 133400, loss: 4.9424e-01, time:   553\n",
      "step: 133500, loss: 4.8901e-01, time:   553\n",
      "step: 133600, loss: 4.8660e-01, time:   554\n",
      "step: 133700, loss: 4.8726e-01, time:   554\n",
      "step: 133800, loss: 4.8794e-01, time:   554\n",
      "step: 133900, loss: 4.9104e-01, time:   555\n",
      "step: 134000, loss: 4.9094e-01, time:   555\n",
      "step: 134100, loss: 4.8715e-01, time:   556\n",
      "step: 134200, loss: 4.8988e-01, time:   556\n",
      "step: 134300, loss: 4.9406e-01, time:   557\n",
      "step: 134400, loss: 4.9155e-01, time:   557\n",
      "step: 134500, loss: 4.8784e-01, time:   557\n",
      "step: 134600, loss: 4.8795e-01, time:   558\n",
      "step: 134700, loss: 4.8922e-01, time:   558\n",
      "step: 134800, loss: 4.8808e-01, time:   559\n",
      "step: 134900, loss: 4.8818e-01, time:   559\n",
      "step: 135000, loss: 4.9276e-01, time:   560\n",
      "step: 135100, loss: 4.8703e-01, time:   560\n",
      "step: 135200, loss: 4.9044e-01, time:   561\n",
      "step: 135300, loss: 4.8638e-01, time:   561\n",
      "step: 135400, loss: 4.9850e-01, time:   562\n",
      "step: 135500, loss: 4.9392e-01, time:   562\n",
      "step: 135600, loss: 5.0305e-01, time:   563\n",
      "step: 135700, loss: 5.0383e-01, time:   563\n",
      "step: 135800, loss: 4.9385e-01, time:   564\n",
      "step: 135900, loss: 4.9273e-01, time:   564\n",
      "step: 136000, loss: 4.9085e-01, time:   565\n",
      "step: 136100, loss: 4.9170e-01, time:   565\n",
      "step: 136200, loss: 4.9242e-01, time:   565\n",
      "step: 136300, loss: 4.8943e-01, time:   566\n",
      "step: 136400, loss: 4.8967e-01, time:   566\n",
      "step: 136500, loss: 4.9072e-01, time:   567\n",
      "step: 136600, loss: 4.8865e-01, time:   567\n",
      "step: 136700, loss: 4.9169e-01, time:   568\n",
      "step: 136800, loss: 4.8754e-01, time:   568\n",
      "step: 136900, loss: 4.8747e-01, time:   569\n",
      "step: 137000, loss: 4.8576e-01, time:   569\n",
      "step: 137100, loss: 4.8687e-01, time:   570\n",
      "step: 137200, loss: 4.8646e-01, time:   570\n",
      "step: 137300, loss: 5.0089e-01, time:   571\n",
      "step: 137400, loss: 4.8762e-01, time:   571\n",
      "step: 137500, loss: 4.8649e-01, time:   572\n",
      "step: 137600, loss: 5.1858e-01, time:   572\n",
      "step: 137700, loss: 5.0787e-01, time:   572\n",
      "step: 137800, loss: 4.8824e-01, time:   573\n",
      "step: 137900, loss: 4.8961e-01, time:   573\n",
      "step: 138000, loss: 4.9172e-01, time:   574\n",
      "step: 138100, loss: 4.9463e-01, time:   574\n",
      "step: 138200, loss: 4.8615e-01, time:   575\n",
      "step: 138300, loss: 4.8803e-01, time:   575\n",
      "step: 138400, loss: 4.9676e-01, time:   576\n",
      "step: 138500, loss: 4.8694e-01, time:   576\n",
      "step: 138600, loss: 4.8760e-01, time:   577\n",
      "step: 138700, loss: 4.8644e-01, time:   577\n",
      "step: 138800, loss: 4.8670e-01, time:   577\n",
      "step: 138900, loss: 4.8681e-01, time:   578\n",
      "step: 139000, loss: 4.9425e-01, time:   578\n",
      "step: 139100, loss: 4.8463e-01, time:   579\n",
      "step: 139200, loss: 4.8657e-01, time:   579\n",
      "step: 139300, loss: 4.8435e-01, time:   580\n",
      "step: 139400, loss: 4.8437e-01, time:   580\n",
      "step: 139500, loss: 4.8485e-01, time:   581\n",
      "step: 139600, loss: 4.8672e-01, time:   581\n",
      "step: 139700, loss: 4.8613e-01, time:   581\n",
      "step: 139800, loss: 4.8502e-01, time:   582\n",
      "step: 139900, loss: 4.8636e-01, time:   582\n",
      "step: 140000, loss: 4.8472e-01, time:   583\n",
      "step: 140100, loss: 4.8427e-01, time:   583\n",
      "step: 140200, loss: 4.8473e-01, time:   584\n",
      "step: 140300, loss: 4.8418e-01, time:   584\n",
      "step: 140400, loss: 4.8407e-01, time:   585\n",
      "step: 140500, loss: 4.8484e-01, time:   585\n",
      "step: 140600, loss: 4.8461e-01, time:   585\n",
      "step: 140700, loss: 4.9002e-01, time:   586\n",
      "step: 140800, loss: 4.8466e-01, time:   586\n",
      "step: 140900, loss: 4.9051e-01, time:   587\n",
      "step: 141000, loss: 4.8947e-01, time:   587\n",
      "step: 141100, loss: 4.8894e-01, time:   588\n",
      "step: 141200, loss: 4.8694e-01, time:   588\n",
      "step: 141300, loss: 4.8551e-01, time:   589\n",
      "step: 141400, loss: 4.8519e-01, time:   589\n",
      "step: 141500, loss: 4.8570e-01, time:   590\n",
      "step: 141600, loss: 4.8467e-01, time:   590\n",
      "step: 141700, loss: 4.8747e-01, time:   590\n",
      "step: 141800, loss: 4.8502e-01, time:   591\n",
      "step: 141900, loss: 4.8402e-01, time:   591\n",
      "step: 142000, loss: 4.9719e-01, time:   592\n",
      "step: 142100, loss: 4.8769e-01, time:   592\n",
      "step: 142200, loss: 4.8401e-01, time:   593\n",
      "step: 142300, loss: 4.8378e-01, time:   593\n",
      "step: 142400, loss: 4.8443e-01, time:   594\n",
      "step: 142500, loss: 4.8466e-01, time:   594\n",
      "step: 142600, loss: 4.8652e-01, time:   595\n",
      "step: 142700, loss: 4.8454e-01, time:   595\n",
      "step: 142800, loss: 4.8662e-01, time:   595\n",
      "step: 142900, loss: 4.8565e-01, time:   596\n",
      "step: 143000, loss: 4.8807e-01, time:   596\n",
      "step: 143100, loss: 4.8468e-01, time:   597\n",
      "step: 143200, loss: 4.8664e-01, time:   597\n",
      "step: 143300, loss: 4.8448e-01, time:   598\n",
      "step: 143400, loss: 4.8449e-01, time:   598\n",
      "step: 143500, loss: 4.8379e-01, time:   599\n",
      "step: 143600, loss: 4.8601e-01, time:   599\n",
      "step: 143700, loss: 4.8599e-01, time:   599\n",
      "step: 143800, loss: 4.8509e-01, time:   600\n",
      "step: 143900, loss: 4.8580e-01, time:   600\n",
      "step: 144000, loss: 4.8535e-01, time:   601\n",
      "step: 144100, loss: 4.8453e-01, time:   601\n",
      "step: 144200, loss: 4.8592e-01, time:   602\n",
      "step: 144300, loss: 4.8371e-01, time:   602\n",
      "step: 144400, loss: 4.8520e-01, time:   603\n",
      "step: 144500, loss: 4.8598e-01, time:   603\n",
      "step: 144600, loss: 4.9340e-01, time:   604\n",
      "step: 144700, loss: 4.9175e-01, time:   604\n",
      "step: 144800, loss: 4.8391e-01, time:   605\n",
      "step: 144900, loss: 4.8497e-01, time:   605\n",
      "step: 145000, loss: 4.8786e-01, time:   605\n",
      "step: 145100, loss: 4.8597e-01, time:   606\n",
      "step: 145200, loss: 4.8471e-01, time:   606\n",
      "step: 145300, loss: 4.8246e-01, time:   607\n",
      "step: 145400, loss: 4.8386e-01, time:   607\n",
      "step: 145500, loss: 4.8595e-01, time:   608\n",
      "step: 145600, loss: 4.8416e-01, time:   608\n",
      "step: 145700, loss: 4.8516e-01, time:   609\n",
      "step: 145800, loss: 4.8529e-01, time:   609\n",
      "step: 145900, loss: 4.8676e-01, time:   610\n",
      "step: 146000, loss: 4.8658e-01, time:   610\n",
      "step: 146100, loss: 4.8613e-01, time:   611\n",
      "step: 146200, loss: 4.8685e-01, time:   611\n",
      "step: 146300, loss: 4.9384e-01, time:   612\n",
      "step: 146400, loss: 4.8459e-01, time:   612\n",
      "step: 146500, loss: 4.9021e-01, time:   612\n",
      "step: 146600, loss: 4.8634e-01, time:   613\n",
      "step: 146700, loss: 4.8523e-01, time:   613\n",
      "step: 146800, loss: 4.8905e-01, time:   614\n",
      "step: 146900, loss: 4.8975e-01, time:   614\n",
      "step: 147000, loss: 4.8585e-01, time:   615\n",
      "step: 147100, loss: 4.8738e-01, time:   615\n",
      "step: 147200, loss: 4.8813e-01, time:   616\n",
      "step: 147300, loss: 4.8458e-01, time:   616\n",
      "step: 147400, loss: 4.8480e-01, time:   617\n",
      "step: 147500, loss: 4.8664e-01, time:   617\n",
      "step: 147600, loss: 4.8307e-01, time:   617\n",
      "step: 147700, loss: 4.8745e-01, time:   618\n",
      "step: 147800, loss: 4.8384e-01, time:   618\n",
      "step: 147900, loss: 4.8629e-01, time:   619\n",
      "step: 148000, loss: 4.9023e-01, time:   619\n",
      "step: 148100, loss: 4.8514e-01, time:   620\n",
      "step: 148200, loss: 4.9400e-01, time:   620\n",
      "step: 148300, loss: 4.8766e-01, time:   621\n",
      "step: 148400, loss: 4.8869e-01, time:   621\n",
      "step: 148500, loss: 4.8470e-01, time:   621\n",
      "step: 148600, loss: 4.8523e-01, time:   622\n",
      "step: 148700, loss: 4.8367e-01, time:   622\n",
      "step: 148800, loss: 4.8339e-01, time:   623\n",
      "step: 148900, loss: 4.8423e-01, time:   623\n",
      "step: 149000, loss: 4.8398e-01, time:   624\n",
      "step: 149100, loss: 4.8319e-01, time:   624\n",
      "step: 149200, loss: 4.8777e-01, time:   625\n",
      "step: 149300, loss: 4.9457e-01, time:   625\n",
      "step: 149400, loss: 4.8587e-01, time:   626\n",
      "step: 149500, loss: 4.9365e-01, time:   626\n",
      "step: 149600, loss: 4.8562e-01, time:   626\n",
      "step: 149700, loss: 4.8809e-01, time:   627\n",
      "step: 149800, loss: 4.8957e-01, time:   627\n",
      "step: 149900, loss: 4.8565e-01, time:   628\n",
      "step: 150000, loss: 5.0201e-01, time:   628\n",
      "step: 150100, loss: 4.8553e-01, time:   629\n",
      "step: 150200, loss: 4.8337e-01, time:   629\n",
      "step: 150300, loss: 4.8706e-01, time:   630\n",
      "step: 150400, loss: 4.8524e-01, time:   630\n",
      "step: 150500, loss: 4.8426e-01, time:   630\n",
      "step: 150600, loss: 4.8580e-01, time:   631\n",
      "step: 150700, loss: 4.8874e-01, time:   631\n",
      "step: 150800, loss: 4.8423e-01, time:   632\n",
      "step: 150900, loss: 4.9106e-01, time:   632\n",
      "step: 151000, loss: 4.8690e-01, time:   633\n",
      "step: 151100, loss: 4.8277e-01, time:   633\n",
      "step: 151200, loss: 4.8224e-01, time:   634\n",
      "step: 151300, loss: 4.8367e-01, time:   634\n",
      "step: 151400, loss: 4.8343e-01, time:   635\n",
      "step: 151500, loss: 4.8171e-01, time:   635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 151600, loss: 4.8273e-01, time:   636\n",
      "step: 151700, loss: 4.8121e-01, time:   636\n",
      "step: 151800, loss: 4.8177e-01, time:   637\n",
      "step: 151900, loss: 4.8150e-01, time:   637\n",
      "step: 152000, loss: 4.9050e-01, time:   637\n",
      "step: 152100, loss: 4.8124e-01, time:   638\n",
      "step: 152200, loss: 4.8201e-01, time:   638\n",
      "step: 152300, loss: 4.8322e-01, time:   639\n",
      "step: 152400, loss: 4.8255e-01, time:   639\n",
      "step: 152500, loss: 4.8337e-01, time:   640\n",
      "step: 152600, loss: 4.8326e-01, time:   640\n",
      "step: 152700, loss: 4.8523e-01, time:   641\n",
      "step: 152800, loss: 4.8409e-01, time:   641\n",
      "step: 152900, loss: 4.8259e-01, time:   641\n",
      "step: 153000, loss: 4.8530e-01, time:   642\n",
      "step: 153100, loss: 4.8756e-01, time:   642\n",
      "step: 153200, loss: 5.6477e-01, time:   643\n",
      "step: 153300, loss: 5.0964e-01, time:   643\n",
      "step: 153400, loss: 5.0144e-01, time:   644\n",
      "step: 153500, loss: 5.0027e-01, time:   644\n",
      "step: 153600, loss: 4.9630e-01, time:   645\n",
      "step: 153700, loss: 4.9916e-01, time:   645\n",
      "step: 153800, loss: 4.9018e-01, time:   645\n",
      "step: 153900, loss: 4.8888e-01, time:   646\n",
      "step: 154000, loss: 4.9297e-01, time:   646\n",
      "step: 154100, loss: 4.9260e-01, time:   647\n",
      "step: 154200, loss: 4.8823e-01, time:   647\n",
      "step: 154300, loss: 4.8830e-01, time:   648\n",
      "step: 154400, loss: 4.8905e-01, time:   648\n",
      "step: 154500, loss: 4.8890e-01, time:   649\n",
      "step: 154600, loss: 4.8838e-01, time:   649\n",
      "step: 154700, loss: 4.8941e-01, time:   649\n",
      "step: 154800, loss: 4.8701e-01, time:   650\n",
      "step: 154900, loss: 4.8716e-01, time:   650\n",
      "step: 155000, loss: 4.8773e-01, time:   651\n",
      "step: 155100, loss: 4.9042e-01, time:   651\n",
      "step: 155200, loss: 4.8682e-01, time:   652\n",
      "step: 155300, loss: 4.8810e-01, time:   652\n",
      "step: 155400, loss: 4.8782e-01, time:   653\n",
      "step: 155500, loss: 4.8611e-01, time:   653\n",
      "step: 155600, loss: 4.8745e-01, time:   653\n",
      "step: 155700, loss: 4.9227e-01, time:   654\n",
      "step: 155800, loss: 4.8707e-01, time:   654\n",
      "step: 155900, loss: 4.9187e-01, time:   655\n",
      "step: 156000, loss: 4.8532e-01, time:   655\n",
      "step: 156100, loss: 4.8908e-01, time:   656\n",
      "step: 156200, loss: 4.8579e-01, time:   656\n",
      "step: 156300, loss: 4.8594e-01, time:   657\n",
      "step: 156400, loss: 4.8398e-01, time:   657\n",
      "step: 156500, loss: 4.8498e-01, time:   657\n",
      "step: 156600, loss: 4.8388e-01, time:   658\n",
      "step: 156700, loss: 4.8384e-01, time:   658\n",
      "step: 156800, loss: 4.8418e-01, time:   659\n",
      "step: 156900, loss: 4.8313e-01, time:   659\n",
      "step: 157000, loss: 4.8765e-01, time:   660\n",
      "step: 157100, loss: 4.8395e-01, time:   660\n",
      "step: 157200, loss: 4.8418e-01, time:   661\n",
      "step: 157300, loss: 4.8338e-01, time:   661\n",
      "step: 157400, loss: 4.9172e-01, time:   661\n",
      "step: 157500, loss: 4.8235e-01, time:   662\n",
      "step: 157600, loss: 4.8316e-01, time:   662\n",
      "step: 157700, loss: 4.8357e-01, time:   663\n",
      "step: 157800, loss: 4.8208e-01, time:   663\n",
      "step: 157900, loss: 4.8258e-01, time:   664\n",
      "step: 158000, loss: 4.8212e-01, time:   664\n",
      "step: 158100, loss: 4.8379e-01, time:   665\n",
      "step: 158200, loss: 4.8259e-01, time:   665\n",
      "step: 158300, loss: 4.8357e-01, time:   666\n",
      "step: 158400, loss: 4.8526e-01, time:   666\n",
      "step: 158500, loss: 4.8431e-01, time:   666\n",
      "step: 158600, loss: 4.8557e-01, time:   667\n",
      "step: 158700, loss: 4.8294e-01, time:   667\n",
      "step: 158800, loss: 4.8312e-01, time:   668\n",
      "step: 158900, loss: 4.8244e-01, time:   668\n",
      "step: 159000, loss: 4.8242e-01, time:   669\n",
      "step: 159100, loss: 4.8560e-01, time:   669\n",
      "step: 159200, loss: 4.8532e-01, time:   670\n",
      "step: 159300, loss: 4.8161e-01, time:   670\n",
      "step: 159400, loss: 4.8453e-01, time:   671\n",
      "step: 159500, loss: 4.8205e-01, time:   671\n",
      "step: 159600, loss: 4.8055e-01, time:   671\n",
      "step: 159700, loss: 4.8358e-01, time:   672\n",
      "step: 159800, loss: 4.8089e-01, time:   672\n",
      "step: 159900, loss: 4.8159e-01, time:   673\n",
      "step: 160000, loss: 4.8168e-01, time:   673\n",
      "step: 160100, loss: 4.8209e-01, time:   674\n",
      "step: 160200, loss: 4.8029e-01, time:   674\n",
      "step: 160300, loss: 4.9038e-01, time:   675\n",
      "step: 160400, loss: 5.6084e-01, time:   675\n",
      "step: 160500, loss: 5.1964e-01, time:   676\n",
      "step: 160600, loss: 5.1788e-01, time:   676\n",
      "step: 160700, loss: 5.1183e-01, time:   676\n",
      "step: 160800, loss: 5.1531e-01, time:   677\n",
      "step: 160900, loss: 5.1417e-01, time:   677\n",
      "step: 161000, loss: 5.0462e-01, time:   678\n",
      "step: 161100, loss: 5.0533e-01, time:   678\n",
      "step: 161200, loss: 5.0390e-01, time:   679\n",
      "step: 161300, loss: 5.1062e-01, time:   679\n",
      "step: 161400, loss: 5.0368e-01, time:   680\n",
      "step: 161500, loss: 5.0170e-01, time:   680\n",
      "step: 161600, loss: 5.0373e-01, time:   681\n",
      "step: 161700, loss: 5.0038e-01, time:   681\n",
      "step: 161800, loss: 5.0128e-01, time:   681\n",
      "step: 161900, loss: 5.0276e-01, time:   682\n",
      "step: 162000, loss: 4.9742e-01, time:   682\n",
      "step: 162100, loss: 4.9653e-01, time:   683\n",
      "step: 162200, loss: 4.9659e-01, time:   683\n",
      "step: 162300, loss: 4.9000e-01, time:   684\n",
      "step: 162400, loss: 4.8905e-01, time:   684\n",
      "step: 162500, loss: 4.8607e-01, time:   685\n",
      "step: 162600, loss: 4.8766e-01, time:   685\n",
      "step: 162700, loss: 4.8554e-01, time:   686\n",
      "step: 162800, loss: 4.8620e-01, time:   686\n",
      "step: 162900, loss: 4.8578e-01, time:   686\n",
      "step: 163000, loss: 4.8609e-01, time:   687\n",
      "step: 163100, loss: 4.8592e-01, time:   687\n",
      "step: 163200, loss: 4.8648e-01, time:   688\n",
      "step: 163300, loss: 4.8858e-01, time:   688\n",
      "step: 163400, loss: 4.8725e-01, time:   689\n",
      "step: 163500, loss: 4.8431e-01, time:   689\n",
      "step: 163600, loss: 4.8621e-01, time:   689\n",
      "step: 163700, loss: 4.8533e-01, time:   690\n",
      "step: 163800, loss: 4.8432e-01, time:   690\n",
      "step: 163900, loss: 4.8600e-01, time:   691\n",
      "step: 164000, loss: 4.8411e-01, time:   691\n",
      "step: 164100, loss: 4.8749e-01, time:   692\n",
      "step: 164200, loss: 4.8539e-01, time:   692\n",
      "step: 164300, loss: 4.8519e-01, time:   693\n",
      "step: 164400, loss: 4.8681e-01, time:   693\n",
      "step: 164500, loss: 4.8584e-01, time:   693\n",
      "step: 164600, loss: 4.8688e-01, time:   694\n",
      "step: 164700, loss: 4.8232e-01, time:   694\n",
      "step: 164800, loss: 4.8492e-01, time:   695\n",
      "step: 164900, loss: 4.9329e-01, time:   695\n",
      "step: 165000, loss: 4.9237e-01, time:   696\n",
      "step: 165100, loss: 4.8520e-01, time:   696\n",
      "step: 165200, loss: 4.8756e-01, time:   697\n",
      "step: 165300, loss: 4.8492e-01, time:   697\n",
      "step: 165400, loss: 4.8358e-01, time:   698\n",
      "step: 165500, loss: 4.8758e-01, time:   698\n",
      "step: 165600, loss: 4.8868e-01, time:   698\n",
      "step: 165700, loss: 4.8357e-01, time:   699\n",
      "step: 165800, loss: 4.8550e-01, time:   699\n",
      "step: 165900, loss: 4.8482e-01, time:   700\n",
      "step: 166000, loss: 4.8476e-01, time:   700\n",
      "step: 166100, loss: 4.8389e-01, time:   701\n",
      "step: 166200, loss: 4.8499e-01, time:   701\n",
      "step: 166300, loss: 4.8637e-01, time:   702\n",
      "step: 166400, loss: 4.8463e-01, time:   702\n",
      "step: 166500, loss: 4.8564e-01, time:   702\n",
      "step: 166600, loss: 4.8521e-01, time:   703\n",
      "step: 166700, loss: 4.8364e-01, time:   703\n",
      "step: 166800, loss: 4.8563e-01, time:   704\n",
      "step: 166900, loss: 4.8223e-01, time:   704\n",
      "step: 167000, loss: 4.8206e-01, time:   705\n",
      "step: 167100, loss: 4.8213e-01, time:   705\n",
      "step: 167200, loss: 4.8106e-01, time:   706\n",
      "step: 167300, loss: 4.8285e-01, time:   706\n",
      "step: 167400, loss: 4.8361e-01, time:   706\n",
      "step: 167500, loss: 4.8473e-01, time:   707\n",
      "step: 167600, loss: 4.8999e-01, time:   707\n",
      "step: 167700, loss: 4.8094e-01, time:   708\n",
      "step: 167800, loss: 4.9281e-01, time:   708\n",
      "step: 167900, loss: 4.8042e-01, time:   709\n",
      "step: 168000, loss: 4.8127e-01, time:   709\n",
      "step: 168100, loss: 4.8680e-01, time:   710\n",
      "step: 168200, loss: 4.9590e-01, time:   710\n",
      "step: 168300, loss: 4.8307e-01, time:   711\n",
      "step: 168400, loss: 4.8313e-01, time:   711\n",
      "step: 168500, loss: 4.8571e-01, time:   711\n",
      "step: 168600, loss: 4.8282e-01, time:   712\n",
      "step: 168700, loss: 4.8538e-01, time:   712\n",
      "step: 168800, loss: 4.8104e-01, time:   713\n",
      "step: 168900, loss: 4.8613e-01, time:   713\n",
      "step: 169000, loss: 4.8140e-01, time:   714\n",
      "step: 169100, loss: 4.8777e-01, time:   714\n",
      "step: 169200, loss: 4.8140e-01, time:   715\n",
      "step: 169300, loss: 4.8324e-01, time:   715\n",
      "step: 169400, loss: 4.8159e-01, time:   715\n",
      "step: 169500, loss: 4.8268e-01, time:   716\n",
      "step: 169600, loss: 4.7986e-01, time:   716\n",
      "step: 169700, loss: 4.8414e-01, time:   717\n",
      "step: 169800, loss: 4.8083e-01, time:   717\n",
      "step: 169900, loss: 4.8215e-01, time:   718\n",
      "step: 170000, loss: 4.8060e-01, time:   718\n",
      "step: 170100, loss: 4.8084e-01, time:   719\n",
      "step: 170200, loss: 4.8128e-01, time:   719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 170300, loss: 4.8018e-01, time:   719\n",
      "step: 170400, loss: 4.8103e-01, time:   720\n",
      "step: 170500, loss: 4.8569e-01, time:   720\n",
      "step: 170600, loss: 4.8220e-01, time:   721\n",
      "step: 170700, loss: 4.8322e-01, time:   721\n",
      "step: 170800, loss: 4.8292e-01, time:   722\n",
      "step: 170900, loss: 4.8597e-01, time:   722\n",
      "step: 171000, loss: 4.8919e-01, time:   723\n",
      "step: 171100, loss: 4.8607e-01, time:   723\n",
      "step: 171200, loss: 4.7983e-01, time:   724\n",
      "step: 171300, loss: 4.8184e-01, time:   724\n",
      "step: 171400, loss: 4.8078e-01, time:   725\n",
      "step: 171500, loss: 4.8182e-01, time:   725\n",
      "step: 171600, loss: 4.8551e-01, time:   726\n",
      "step: 171700, loss: 4.8204e-01, time:   726\n",
      "step: 171800, loss: 4.8704e-01, time:   727\n",
      "step: 171900, loss: 4.8116e-01, time:   727\n",
      "step: 172000, loss: 4.9300e-01, time:   727\n",
      "step: 172100, loss: 4.8308e-01, time:   728\n",
      "step: 172200, loss: 4.8231e-01, time:   729\n",
      "step: 172300, loss: 4.8462e-01, time:   729\n",
      "step: 172400, loss: 4.8256e-01, time:   730\n",
      "step: 172500, loss: 4.8088e-01, time:   730\n",
      "step: 172600, loss: 4.9331e-01, time:   730\n",
      "step: 172700, loss: 4.8317e-01, time:   731\n",
      "step: 172800, loss: 4.8245e-01, time:   731\n",
      "step: 172900, loss: 4.8460e-01, time:   732\n",
      "step: 173000, loss: 4.8112e-01, time:   732\n",
      "step: 173100, loss: 4.8067e-01, time:   733\n",
      "step: 173200, loss: 4.7893e-01, time:   733\n",
      "step: 173300, loss: 4.8407e-01, time:   734\n",
      "step: 173400, loss: 4.8199e-01, time:   734\n",
      "step: 173500, loss: 4.8435e-01, time:   735\n",
      "step: 173600, loss: 4.7835e-01, time:   735\n",
      "step: 173700, loss: 4.8503e-01, time:   735\n",
      "step: 173800, loss: 4.8293e-01, time:   736\n",
      "step: 173900, loss: 4.8348e-01, time:   736\n",
      "step: 174000, loss: 4.8148e-01, time:   737\n",
      "step: 174100, loss: 4.7890e-01, time:   737\n",
      "step: 174200, loss: 4.8366e-01, time:   738\n",
      "step: 174300, loss: 4.7830e-01, time:   738\n",
      "step: 174400, loss: 4.7896e-01, time:   739\n",
      "step: 174500, loss: 4.7959e-01, time:   739\n",
      "step: 174600, loss: 4.8160e-01, time:   739\n",
      "step: 174700, loss: 4.8255e-01, time:   740\n",
      "step: 174800, loss: 4.8278e-01, time:   740\n",
      "step: 174900, loss: 4.7961e-01, time:   741\n",
      "step: 175000, loss: 4.8828e-01, time:   741\n",
      "step: 175100, loss: 4.8462e-01, time:   742\n",
      "step: 175200, loss: 4.8641e-01, time:   742\n",
      "step: 175300, loss: 4.8091e-01, time:   743\n",
      "step: 175400, loss: 4.8358e-01, time:   743\n",
      "step: 175500, loss: 4.8026e-01, time:   744\n",
      "step: 175600, loss: 4.8221e-01, time:   744\n",
      "step: 175700, loss: 4.7913e-01, time:   744\n",
      "step: 175800, loss: 4.7918e-01, time:   745\n",
      "step: 175900, loss: 4.7903e-01, time:   745\n",
      "step: 176000, loss: 4.7998e-01, time:   746\n",
      "step: 176100, loss: 4.8300e-01, time:   746\n",
      "step: 176200, loss: 4.7908e-01, time:   747\n",
      "step: 176300, loss: 4.8022e-01, time:   747\n",
      "step: 176400, loss: 4.8607e-01, time:   748\n",
      "step: 176500, loss: 4.8056e-01, time:   748\n",
      "step: 176600, loss: 4.8428e-01, time:   749\n",
      "step: 176700, loss: 4.8197e-01, time:   749\n",
      "step: 176800, loss: 4.7933e-01, time:   750\n",
      "step: 176900, loss: 4.8150e-01, time:   750\n",
      "step: 177000, loss: 4.7967e-01, time:   751\n",
      "step: 177100, loss: 4.8009e-01, time:   751\n",
      "step: 177200, loss: 4.7840e-01, time:   752\n",
      "step: 177300, loss: 4.7911e-01, time:   752\n",
      "step: 177400, loss: 4.8362e-01, time:   753\n",
      "step: 177500, loss: 4.8511e-01, time:   753\n",
      "step: 177600, loss: 4.8360e-01, time:   754\n",
      "step: 177700, loss: 4.7856e-01, time:   754\n",
      "step: 177800, loss: 4.7871e-01, time:   754\n",
      "step: 177900, loss: 4.7897e-01, time:   755\n",
      "step: 178000, loss: 4.7801e-01, time:   755\n",
      "step: 178100, loss: 4.7908e-01, time:   756\n",
      "step: 178200, loss: 4.7790e-01, time:   756\n",
      "step: 178300, loss: 4.8040e-01, time:   757\n",
      "step: 178400, loss: 4.7832e-01, time:   757\n",
      "step: 178500, loss: 4.7792e-01, time:   758\n",
      "step: 178600, loss: 4.8097e-01, time:   758\n",
      "step: 178700, loss: 4.7896e-01, time:   759\n",
      "step: 178800, loss: 4.8509e-01, time:   759\n",
      "step: 178900, loss: 4.8228e-01, time:   760\n",
      "step: 179000, loss: 4.8758e-01, time:   760\n",
      "step: 179100, loss: 4.8108e-01, time:   761\n",
      "step: 179200, loss: 4.8412e-01, time:   761\n",
      "step: 179300, loss: 4.7770e-01, time:   762\n",
      "step: 179400, loss: 4.7897e-01, time:   762\n",
      "step: 179500, loss: 4.8021e-01, time:   763\n",
      "step: 179600, loss: 4.7760e-01, time:   763\n",
      "step: 179700, loss: 4.8349e-01, time:   764\n",
      "step: 179800, loss: 4.7838e-01, time:   764\n",
      "step: 179900, loss: 4.7987e-01, time:   765\n",
      "step: 180000, loss: 4.7959e-01, time:   765\n",
      "step: 180100, loss: 4.7779e-01, time:   766\n",
      "step: 180200, loss: 4.7759e-01, time:   766\n",
      "step: 180300, loss: 4.7886e-01, time:   767\n",
      "step: 180400, loss: 4.7758e-01, time:   767\n",
      "step: 180500, loss: 4.8220e-01, time:   768\n",
      "step: 180600, loss: 4.7811e-01, time:   768\n",
      "step: 180700, loss: 4.7963e-01, time:   769\n",
      "step: 180800, loss: 4.7826e-01, time:   769\n",
      "step: 180900, loss: 4.7945e-01, time:   769\n",
      "step: 181000, loss: 4.8797e-01, time:   770\n",
      "step: 181100, loss: 4.8470e-01, time:   770\n",
      "step: 181200, loss: 4.9037e-01, time:   771\n",
      "step: 181300, loss: 4.9626e-01, time:   771\n",
      "step: 181400, loss: 4.8187e-01, time:   772\n",
      "step: 181500, loss: 4.8409e-01, time:   772\n",
      "step: 181600, loss: 4.9351e-01, time:   773\n",
      "step: 181700, loss: 4.8466e-01, time:   773\n",
      "step: 181800, loss: 4.8167e-01, time:   774\n",
      "step: 181900, loss: 4.8734e-01, time:   774\n",
      "step: 182000, loss: 4.8383e-01, time:   775\n",
      "step: 182100, loss: 4.8165e-01, time:   775\n",
      "step: 182200, loss: 4.8574e-01, time:   775\n",
      "step: 182300, loss: 5.0472e-01, time:   776\n",
      "step: 182400, loss: 4.8440e-01, time:   776\n",
      "step: 182500, loss: 4.8260e-01, time:   777\n",
      "step: 182600, loss: 4.8258e-01, time:   777\n",
      "step: 182700, loss: 4.7897e-01, time:   778\n",
      "step: 182800, loss: 4.8480e-01, time:   778\n",
      "step: 182900, loss: 4.8332e-01, time:   779\n",
      "step: 183000, loss: 4.7971e-01, time:   779\n",
      "step: 183100, loss: 4.8359e-01, time:   779\n",
      "step: 183200, loss: 4.8209e-01, time:   780\n",
      "step: 183300, loss: 4.8089e-01, time:   780\n",
      "step: 183400, loss: 4.9323e-01, time:   781\n",
      "step: 183500, loss: 4.8372e-01, time:   781\n",
      "step: 183600, loss: 4.8204e-01, time:   782\n",
      "step: 183700, loss: 4.8101e-01, time:   782\n",
      "step: 183800, loss: 4.8312e-01, time:   783\n",
      "step: 183900, loss: 4.8195e-01, time:   783\n",
      "step: 184000, loss: 4.8692e-01, time:   784\n",
      "step: 184100, loss: 4.8480e-01, time:   784\n",
      "step: 184200, loss: 4.8308e-01, time:   785\n",
      "step: 184300, loss: 4.8812e-01, time:   785\n",
      "step: 184400, loss: 4.7707e-01, time:   786\n",
      "step: 184500, loss: 4.7773e-01, time:   786\n",
      "step: 184600, loss: 4.8343e-01, time:   787\n",
      "step: 184700, loss: 4.8220e-01, time:   787\n",
      "step: 184800, loss: 4.7992e-01, time:   787\n",
      "step: 184900, loss: 4.8840e-01, time:   788\n",
      "step: 185000, loss: 4.8755e-01, time:   788\n",
      "step: 185100, loss: 4.8048e-01, time:   789\n",
      "step: 185200, loss: 4.7788e-01, time:   789\n",
      "step: 185300, loss: 4.8038e-01, time:   790\n",
      "step: 185400, loss: 4.7931e-01, time:   790\n",
      "step: 185500, loss: 4.8007e-01, time:   791\n",
      "step: 185600, loss: 4.8968e-01, time:   791\n",
      "step: 185700, loss: 4.8168e-01, time:   792\n",
      "step: 185800, loss: 4.8300e-01, time:   792\n",
      "step: 185900, loss: 4.8175e-01, time:   792\n",
      "step: 186000, loss: 4.8574e-01, time:   793\n",
      "step: 186100, loss: 4.8308e-01, time:   793\n",
      "step: 186200, loss: 4.8292e-01, time:   794\n",
      "step: 186300, loss: 4.8311e-01, time:   794\n",
      "step: 186400, loss: 4.8164e-01, time:   795\n",
      "step: 186500, loss: 4.8329e-01, time:   795\n",
      "step: 186600, loss: 4.9641e-01, time:   796\n",
      "step: 186700, loss: 4.8819e-01, time:   796\n",
      "step: 186800, loss: 4.8296e-01, time:   797\n",
      "step: 186900, loss: 4.8181e-01, time:   797\n",
      "step: 187000, loss: 4.8156e-01, time:   797\n",
      "step: 187100, loss: 4.8124e-01, time:   798\n",
      "step: 187200, loss: 4.7772e-01, time:   798\n",
      "step: 187300, loss: 4.8028e-01, time:   799\n",
      "step: 187400, loss: 4.8114e-01, time:   799\n",
      "step: 187500, loss: 4.7904e-01, time:   800\n",
      "step: 187600, loss: 4.7911e-01, time:   800\n",
      "step: 187700, loss: 4.8027e-01, time:   801\n",
      "step: 187800, loss: 4.8190e-01, time:   801\n",
      "step: 187900, loss: 4.7850e-01, time:   801\n",
      "step: 188000, loss: 4.8211e-01, time:   802\n",
      "step: 188100, loss: 4.8014e-01, time:   802\n",
      "step: 188200, loss: 4.8202e-01, time:   803\n",
      "step: 188300, loss: 4.8048e-01, time:   803\n",
      "step: 188400, loss: 4.7939e-01, time:   804\n",
      "step: 188500, loss: 4.8142e-01, time:   804\n",
      "step: 188600, loss: 4.8037e-01, time:   805\n",
      "step: 188700, loss: 4.8457e-01, time:   805\n",
      "step: 188800, loss: 4.7928e-01, time:   805\n",
      "step: 188900, loss: 4.9763e-01, time:   806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 189000, loss: 4.8024e-01, time:   806\n",
      "step: 189100, loss: 4.7909e-01, time:   807\n",
      "step: 189200, loss: 4.8519e-01, time:   807\n",
      "step: 189300, loss: 4.7915e-01, time:   808\n",
      "step: 189400, loss: 4.7968e-01, time:   808\n",
      "step: 189500, loss: 4.7931e-01, time:   809\n",
      "step: 189600, loss: 4.7873e-01, time:   809\n",
      "step: 189700, loss: 4.8269e-01, time:   809\n",
      "step: 189800, loss: 4.7719e-01, time:   810\n",
      "step: 189900, loss: 4.8152e-01, time:   810\n",
      "step: 190000, loss: 4.9771e-01, time:   811\n",
      "step: 190100, loss: 4.8441e-01, time:   811\n",
      "step: 190200, loss: 4.8666e-01, time:   812\n",
      "step: 190300, loss: 4.8157e-01, time:   812\n",
      "step: 190400, loss: 4.8834e-01, time:   813\n",
      "step: 190500, loss: 4.8190e-01, time:   813\n",
      "step: 190600, loss: 4.8001e-01, time:   814\n",
      "step: 190700, loss: 4.8048e-01, time:   814\n",
      "step: 190800, loss: 4.7956e-01, time:   815\n",
      "step: 190900, loss: 4.8189e-01, time:   815\n",
      "step: 191000, loss: 4.8240e-01, time:   816\n",
      "step: 191100, loss: 4.7981e-01, time:   816\n",
      "step: 191200, loss: 4.8031e-01, time:   817\n",
      "step: 191300, loss: 4.8230e-01, time:   817\n",
      "step: 191400, loss: 4.8899e-01, time:   818\n",
      "step: 191500, loss: 4.8321e-01, time:   818\n",
      "step: 191600, loss: 4.7935e-01, time:   819\n",
      "step: 191700, loss: 4.8179e-01, time:   819\n",
      "step: 191800, loss: 4.9503e-01, time:   820\n",
      "step: 191900, loss: 4.8012e-01, time:   820\n",
      "step: 192000, loss: 4.8303e-01, time:   820\n",
      "step: 192100, loss: 4.7977e-01, time:   821\n",
      "step: 192200, loss: 5.1087e-01, time:   821\n",
      "step: 192300, loss: 4.8145e-01, time:   822\n",
      "step: 192400, loss: 4.8197e-01, time:   822\n",
      "step: 192500, loss: 4.9042e-01, time:   823\n",
      "step: 192600, loss: 4.8116e-01, time:   823\n",
      "step: 192700, loss: 4.8302e-01, time:   824\n",
      "step: 192800, loss: 4.7945e-01, time:   824\n",
      "step: 192900, loss: 4.8062e-01, time:   824\n",
      "step: 193000, loss: 4.7770e-01, time:   825\n",
      "step: 193100, loss: 4.7837e-01, time:   825\n",
      "step: 193200, loss: 4.7794e-01, time:   826\n",
      "step: 193300, loss: 4.8405e-01, time:   826\n",
      "step: 193400, loss: 4.8631e-01, time:   827\n",
      "step: 193500, loss: 4.8599e-01, time:   827\n",
      "step: 193600, loss: 4.7675e-01, time:   827\n",
      "step: 193700, loss: 5.1093e-01, time:   828\n",
      "step: 193800, loss: 5.0604e-01, time:   828\n",
      "step: 193900, loss: 5.0516e-01, time:   829\n",
      "step: 194000, loss: 5.0633e-01, time:   829\n",
      "step: 194100, loss: 5.0678e-01, time:   830\n",
      "step: 194200, loss: 5.0619e-01, time:   830\n",
      "step: 194300, loss: 5.0248e-01, time:   831\n",
      "step: 194400, loss: 5.0204e-01, time:   831\n",
      "step: 194500, loss: 5.0241e-01, time:   832\n",
      "step: 194600, loss: 5.0698e-01, time:   832\n",
      "step: 194700, loss: 5.0300e-01, time:   832\n",
      "step: 194800, loss: 5.0542e-01, time:   833\n",
      "step: 194900, loss: 5.0377e-01, time:   833\n",
      "step: 195000, loss: 5.0397e-01, time:   834\n",
      "step: 195100, loss: 5.0354e-01, time:   834\n",
      "step: 195200, loss: 5.0450e-01, time:   835\n",
      "step: 195300, loss: 5.0125e-01, time:   835\n",
      "step: 195400, loss: 5.0655e-01, time:   836\n",
      "step: 195500, loss: 5.0089e-01, time:   836\n",
      "step: 195600, loss: 5.0431e-01, time:   837\n",
      "step: 195700, loss: 5.0559e-01, time:   837\n",
      "step: 195800, loss: 5.0096e-01, time:   837\n",
      "step: 195900, loss: 5.1033e-01, time:   838\n",
      "step: 196000, loss: 5.1022e-01, time:   838\n",
      "step: 196100, loss: 5.1184e-01, time:   839\n",
      "step: 196200, loss: 5.0988e-01, time:   839\n",
      "step: 196300, loss: 5.0932e-01, time:   840\n",
      "step: 196400, loss: 4.9969e-01, time:   840\n",
      "step: 196500, loss: 4.9934e-01, time:   841\n",
      "step: 196600, loss: 5.0102e-01, time:   841\n",
      "step: 196700, loss: 5.0090e-01, time:   841\n",
      "step: 196800, loss: 4.9673e-01, time:   842\n",
      "step: 196900, loss: 4.9844e-01, time:   842\n",
      "step: 197000, loss: 4.9449e-01, time:   843\n",
      "step: 197100, loss: 4.9478e-01, time:   843\n",
      "step: 197200, loss: 4.9695e-01, time:   844\n",
      "step: 197300, loss: 4.9614e-01, time:   844\n",
      "step: 197400, loss: 4.9457e-01, time:   845\n",
      "step: 197500, loss: 4.9375e-01, time:   845\n",
      "step: 197600, loss: 4.9448e-01, time:   846\n",
      "step: 197700, loss: 4.9374e-01, time:   846\n",
      "step: 197800, loss: 4.9393e-01, time:   847\n",
      "step: 197900, loss: 4.9362e-01, time:   847\n",
      "step: 198000, loss: 4.9531e-01, time:   847\n",
      "step: 198100, loss: 4.9335e-01, time:   848\n",
      "step: 198200, loss: 4.9671e-01, time:   848\n",
      "step: 198300, loss: 4.9472e-01, time:   849\n",
      "step: 198400, loss: 4.9605e-01, time:   849\n",
      "step: 198500, loss: 4.9368e-01, time:   850\n",
      "step: 198600, loss: 4.9835e-01, time:   850\n",
      "step: 198700, loss: 4.9783e-01, time:   851\n",
      "step: 198800, loss: 4.9349e-01, time:   851\n",
      "step: 198900, loss: 4.9647e-01, time:   852\n",
      "step: 199000, loss: 4.9492e-01, time:   852\n",
      "step: 199100, loss: 4.9290e-01, time:   853\n",
      "step: 199200, loss: 4.9533e-01, time:   853\n",
      "step: 199300, loss: 5.0376e-01, time:   854\n",
      "step: 199400, loss: 4.9672e-01, time:   854\n",
      "step: 199500, loss: 4.9620e-01, time:   854\n",
      "step: 199600, loss: 4.9412e-01, time:   855\n",
      "step: 199700, loss: 4.9406e-01, time:   855\n",
      "step: 199800, loss: 4.9438e-01, time:   856\n",
      "step: 199900, loss: 4.9507e-01, time:   856\n",
      "[-0.04581124  0.2225242  -0.01241453 ... -0.00825688 -0.01171815\n",
      " -0.00704968]\n"
     ]
    }
   ],
   "source": [
    "class framework(object):\n",
    "    \"\"\"The fully - connected neural network model.\"\"\"\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess\n",
    "    # parameters for the function we want to approximate\n",
    "        self.domain_bound = 50   # boundary value M\n",
    "        self.terminal_time = 1\n",
    "        \n",
    "    # parameters for the algorithm\n",
    "        self.n_simple_train1 = 125 # number samples in training set \n",
    "        self.n_simple_train2 = 100\n",
    "        self.n_simple_train3 = 100\n",
    "        self.n_simple_train4 = 100\n",
    "        self.n_simple_valid = 2000 # number samples in validation set\n",
    "\n",
    "        self.hidden_neuron = 50  #number of neurons in each layer \n",
    "        self.n_neuron = [1, self.hidden_neuron, self.hidden_neuron, 1] \n",
    "        #structure of of neural net\n",
    "        \n",
    "    \n",
    "        self.n_maxstep = 200000 # number of maximum iteration\n",
    "        self.n_displaystep = 100 # every n_displaystep steps, we output some information\n",
    "        self.picdisplaystep= 200 # every picdisplaystep steps, we output graph\n",
    "        self.learning_rate = 5e-4\n",
    "        \n",
    "        # some basic constants and variables\n",
    "        self._extra_train_ops = []\n",
    "        self.tolerence = 5e-2\n",
    "        \n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        trainable_vars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.loss, trainable_vars)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        apply_op = \\\n",
    "                optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        train_ops = [apply_op] + self._extra_train_ops\n",
    "        self.train_op = tf.group(* train_ops)\n",
    "\n",
    "        self.loss_history = []\n",
    "        self.time_history = []\n",
    "        \n",
    "#=============================================================================\n",
    "# You can change the code in this block if needed. But make sure always feed \n",
    "# neural net what it needs. \n",
    "        x_valid1 = self.sample_point_X1(self.n_simple_valid)\n",
    "        x_valid2 = self.sample_point_X2(self.n_simple_valid)\n",
    "        x_valid3 = self.sample_point_X3(self.n_simple_valid)\n",
    "        x_valid4 = self.sample_point_X4(self.n_simple_valid)\n",
    "        feed_dict_valid = {self.Xi: x_valid1, self.Xb1: x_valid2, self.Xt: x_valid3, self.Xb2: x_valid4, self.is_training: False}\n",
    "#=============================================================================\n",
    "        print(\"before initialized\")\n",
    "        \n",
    "        # initialization\n",
    "        step = 1\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        temp_loss = self.sess.run(self.loss,\n",
    "                                  feed_dict = feed_dict_valid)\n",
    "        self.loss_history.append(temp_loss)\n",
    "        self.time_history.append(0)\n",
    "        \n",
    "        print(\"step: %5u, loss: %.4e,\" % \\\n",
    "              (0, temp_loss))\n",
    "        \n",
    "        #x_test_gradient = np.array([[0],[1],[2],[3]])\n",
    "        #gradient_test = self.sess.run(self.gradient_test,\n",
    "                                     #feed_dict = {self.X: x_test_gradient,\n",
    "                                                 #self.is_training: False})\n",
    "        \n",
    "        # begin sgd iteration\n",
    "        # for iii in range (self.n_maxstep + 2):\n",
    "        while (self.loss_history[-1] > self.tolerence) and (step < self.n_maxstep) :\n",
    "             x_train1 = self.sample_point_X1(self.n_simple_train1)\n",
    "             x_train2 = self.sample_point_X2(self.n_simple_train2)\n",
    "             x_train3 = self.sample_point_X3(self.n_simple_train3)\n",
    "             x_train4 = self.sample_point_X4(self.n_simple_train4)\n",
    "                \n",
    "             self.sess.run(self.train_op,\n",
    "                           feed_dict = {self.Xi: x_train1, self.Xb1: x_train2, self.Xt: x_train3, self.Xb2: x_train4,\n",
    "                                       self.is_training: True})\n",
    "             if step % self.n_displaystep == 0:\n",
    "                 temp_loss = self.sess.run(self.loss,\n",
    "                                           feed_dict = feed_dict_valid)\n",
    "                 self.loss_history.append(temp_loss)\n",
    "                 self.time_history.append(time.time() - start_time + self.t_bd)\n",
    "                 print (\"step: %5u, loss: %.4e, time: %5u\" % \\\n",
    "                        (step , temp_loss, time.time() - start_time))\n",
    "\n",
    "             #if step % self .picdisplaystep == 0:\n",
    "                     #approximate_f = self.sess.run(self.output,\n",
    "                                                   #feed_dict = feed_dict_valid)                     \n",
    "                     #plt.scatter(x_valid, approximate_f, label = 'approximation of neural net')\n",
    "                     #plt.scatter(x_valid, self.f(x_valid), label = 'real function f')\n",
    "                     #plt.legend()\n",
    "                     #plt.show()\n",
    "             step += 1\n",
    "\n",
    "        x1 = [0, 10]\n",
    "        self.outputsample = self.sess.run(self.output, \n",
    "                                          feed_dict = {self.Xi: x_valid1, self.Xb1: x_valid2, \n",
    "                                                       self.Xt: x_valid3, self.Xb2: x_valid4, self.is_training: False})\n",
    "#=============================================================================        \n",
    "# You will need to rewrite this block. \n",
    "    def build(self):\n",
    "        start_time = time.time()\n",
    "        self.Xi = tf.placeholder(tf.float64, [None, 2], name = 'Xi') # interior\n",
    "        self.Xb1 = tf.placeholder(tf.float64, [None, 2], name = 'Xb1') # boundary\n",
    "        self.Xt = tf.placeholder(tf.float64, [None, 2], name = 'Xt') # initial\n",
    "        self.Xb2 = tf.placeholder(tf.float64, [None, 2], name = 'Xb2') \n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        \n",
    "# the variable f_ is output of neural net given certain input.       \n",
    "        c_interior = self._one_time_net(self.Xi, 'c')[:,0]\n",
    "        c_boundary1 = self._one_time_net(self.Xb1, 'c')[:,0]\n",
    "        c_boundary2 = self._one_time_net(self.Xb2, 'c')[:,0]\n",
    "        c_initial = self._one_time_net(self.Xt, 'c')[:,0]\n",
    "        \n",
    "# loss function here is just L2 norm of difference of this two function.\n",
    "        c_interior_dt = tf.gradients(c_interior, self.Xi)[0][:,0]\n",
    "        c_interior_dx = tf.gradients(c_interior, self.Xi)[0][:,1]\n",
    "        c_interior_dxx = tf.gradients(c_interior_dx, self.Xi)[0][:,1]\n",
    "        #c_boundary_dx = tf.gradients(c_boundary, self.Xb)[0][:,1]\n",
    "        \n",
    "        loss_initial = c_initial - tf.math.sin(2*np.pi*self.Xt[:,1])\n",
    "        loss_boundary1 = (c_boundary1 -1)\n",
    "        loss_boundary2 = (c_boundary2-1)\n",
    "        loss_interior = c_interior_dt - (1.0/(4.0*np.pi*np.pi))*c_interior_dxx\n",
    "    \n",
    "        self.loss = tf.reduce_mean((loss_initial)**2) + tf.reduce_mean((loss_boundary1)**2) + tf.reduce_mean((loss_interior)**2)+tf.reduce_mean((loss_boundary2)**2)\n",
    "        \n",
    "# test of tf.gradient function\n",
    "        #self.gradient_test = tf.gradients(self.f(self.X), self.X)\n",
    "#=============================================================================  \n",
    "        self.output = c_interior\n",
    "        self.t_bd = time.time() - start_time        \n",
    "        print('end of build')\n",
    "#=============================================================================        \n",
    "# this function define how we sample points in domain\n",
    "    def sample_point_X1(self, n_sample1): #interior\n",
    "        t = np.random.uniform(low = 0, high = self.terminal_time, size = (n_sample1, 1))\n",
    "        x = np.random.uniform(low = 0, high = self.domain_bound, size = (n_sample1, 1))\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "    def sample_point_X2(self, n_sample2): # boundary x=M\n",
    "        t = np.random.uniform(low = 0, high = self.terminal_time, size = (n_sample2, 1))\n",
    "        x = np.ones((n_sample2, 1)) * self.domain_bound\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    def sample_point_X4(self, n_sample4): # boundary x=0\n",
    "        t = np.random.uniform(low = 0, high = self.terminal_time, size = (n_sample4, 1))\n",
    "        x = np.ones((n_sample4, 1)) * 0\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "    def sample_point_X3(self, n_sample3): # initial t=0\n",
    "        t = np.ones((n_sample3, 1)) * 0\n",
    "        x = np.random.uniform(low = 0, high = self.domain_bound, size = (n_sample3, 1))\n",
    "        X = np.hstack((t, x))\n",
    "        X\n",
    "        return X\n",
    "    \n",
    "# this is the function we want to approximate\n",
    "#    def f(self, x):\n",
    "#        return x**3\n",
    "    \n",
    "# you can change number of layer or activation function if needed\n",
    "    def _one_time_net(self, x, name):\n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            layer1 = self._one_layer(x, self.n_neuron[1], activation_fn = tf.nn.elu, name = 'layer1')\n",
    "            layer2 = self._one_layer(layer1, self.n_neuron[2], activation_fn = tf.nn.relu,  name = 'layer2')\n",
    "            layer3 = self._one_layer(layer2, self.n_neuron[2], activation_fn = tf.nn.relu,  name = 'layer2')\n",
    "            u = self._one_layer(layer3, self.n_neuron[3], activation_fn = None ,name ='final')\n",
    "        return u\n",
    "\n",
    "# you can change initialization of weight in each layer if needed\n",
    "    def _one_layer(self, input_, out_sz,\n",
    "                   activation_fn = None,\n",
    "                   std =1.0, name = 'linear'):\n",
    "        with tf.variable_scope(name,):\n",
    "            shape = input_.get_shape().as_list()\n",
    "            w = tf.get_variable('Matrix',\n",
    "                                [shape[1], out_sz], tf.float64,\n",
    "                                norm_init(stddev = \\\n",
    "                                          std / np.sqrt(shape[1] + out_sz)))\n",
    "            hidden = tf.matmul(input_, w)\n",
    "        if activation_fn != None:\n",
    "            return activation_fn(hidden)\n",
    "        else:\n",
    "            return hidden\n",
    "\n",
    "#if __name__ == '__main__ ':\n",
    "#    np.random.seed(1)\n",
    "#    main()\n",
    "#main()\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.set_random_seed(1)\n",
    "    print(\"begin to solve approximation\")\n",
    "    model = framework(sess)\n",
    "    model.build()\n",
    "    model.train()\n",
    "    output = np.zeros((len(model.loss_history), 3))\n",
    "    output[:,0] = np.arange(len(model.loss_history)) \\\n",
    "    * model.n_displaystep\n",
    "    output[:,1] = model.loss_history\n",
    "    output[:,2] = model.time_history\n",
    "    outputsample = model.outputsample\n",
    "    print(outputsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Convergence rate (time)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [125,110 ,95 ,80, 75 ,60 ,50, 40]\n",
    "y = [973 ,960, 878, 856, 850 ,848 ,853, 830]\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Number of data points\")\n",
    "plt.ylabel(\"Convergence rate (time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
